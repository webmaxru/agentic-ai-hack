{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Azure OpenAI Multimodal Model\n",
    "\n",
    "This notebook handles:\n",
    "1. **Document Upload** - Upload policy and claims documents to Azure Blob Storage\n",
    "2. **Text Processing** - Process .md files using GPT-4o\n",
    "3. **OCR Processing** - Extract text from images using GPT-4o vision capabilities\n",
    "4. **Text Enhancement** - Clean and prepare documents for vectorization\n",
    "\n",
    "## Prerequisites\n",
    "- Azure Blob Storage account created\n",
    "- Azure OpenAI service with GPT-4o model deployed\n",
    "- Environment variables configured in `.env` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's start with handling the import of our libraries and load the `.env` variables that we have saved in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell initializes Azure service clients - it creates connections to both Azure Blob Storage and Azure OpenAI services using the configuration variables from your .env file. The initialize_clients() function sets up the BlobServiceClient for file storage operations and the AzureOpenAI client for GPT-4o processing, with error handling to ensure both services are properly connected before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "üìÅ Policies directory: data\\policies\n",
      "üìÅ Statements directory: data\\statements\n",
      "üìÅ Claims directory: data\\claims\n",
      "ü§ñ OpenAI Deployment: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    AZURE_STORAGE_ACCOUNT_NAME = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')\n",
    "    AZURE_STORAGE_ACCOUNT_KEY = os.getenv('AZURE_STORAGE_ACCOUNT_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1-mini')\n",
    "        \n",
    "    # Cosmos DB configuration\n",
    "    COSMOS_ENDPOINT = os.getenv('COSMOS_ENDPOINT')\n",
    "    COSMOS_KEY = os.getenv('COSMOS_KEY')\n",
    "    COSMOS_DATABASE = 'insurance_claims'\n",
    "    COSMOS_CONTAINER = 'crash_reports'\n",
    "    \n",
    "    # Container names\n",
    "    POLICIES_CONTAINER = 'policies'\n",
    "    CLAIMS_CONTAINER = 'claims'\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    STATEMENTS_CONTAINER = 'statements'\n",
    "    \n",
    "    # Local data paths\n",
    "    DATA_DIR = Path('data')\n",
    "    POLICIES_DIR = DATA_DIR / 'policies'\n",
    "    CLAIMS_DIR = DATA_DIR / 'claims'\n",
    "    STATEMENTS_DIR = DATA_DIR / 'statements'\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables - please add these to your .env file:\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"üìÅ Policies directory: {Config.POLICIES_DIR}\")\n",
    "    print(f\"üìÅ Statements directory: {Config.STATEMENTS_DIR}\")\n",
    "    print(f\"üìÅ Claims directory: {Config.CLAIMS_DIR}\")\n",
    "    print(f\"ü§ñ OpenAI Deployment: {Config.AZURE_OPENAI_DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_aMTmWqCUL9YECCIiEEa7ncYx\n",
      "Created thread, ID: thread_TjPwTIMlBiLTdtnvrhrsXwYO\n",
      "Created message, ID: msg_qYZoJEjWAiOzd6Iao7JC9lCJ\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Motorcycle insurance policies typically offer a variety of coverage options, which may include:\n",
      "\n",
      "1. **Liability Coverage:** Covers damages to others for which the policyholder is legally responsible, including bodily injury and property damage.\n",
      "  \n",
      "2. **Collision Coverage:** Covers damages to the motorcycle resulting from a collision with another vehicle or object, regardless of fault.\n",
      "\n",
      "3. **Comprehensive Coverage:** Covers non-collision related incidents such as theft, vandalism, natural disasters, and animal strikes.\n",
      "\n",
      "4. **Uninsured/Underinsured Motorist Coverage:** Provides protection if the motorcycle is damaged by a driver with insufficient insurance or no insurance at all.\n",
      "\n",
      "5. **Medical Payments Coverage:** Covers medical expenses for the rider and any passengers injured in an accident, regardless of fault.\n",
      "\n",
      "6. **Personal Injury Protection (PIP):** Similar to medical payments but may also cover lost wages and other related expenses following an accident.\n",
      "\n",
      "7. **Accessory Coverage:** Covers modifications or accessories added to the motorcycle that aren't covered under standard policies.\n",
      "\n",
      "8. **Roadside Assistance:** Offers services like towing, tire changes, and lockout assistance.\n",
      "\n",
      "Each of these options comes with specific limits, deductibles, and possible exclusions, which should be reviewed in detail within the actual policy documents.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve endpoint and model deployment name from environment variables\n",
    "project_endpoint = \"https://msagthack-aifoundry-kl2yy7velhg4u.services.ai.azure.com/api/projects/msagthack-aiproject-kl2yy7velhg4u\"  # Ensure the PROJECT_ENDPOINT environment variable is set\n",
    "model_deployment_name = \"gpt-4o-mini\"  # Ensure the MODEL_DEPLOYMENT_NAME environment variable is set\n",
    "\n",
    "# Initialize the AIProjectClient with the endpoint and credentials\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(exclude_interactive_browser_credential=False),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "with project_client:\n",
    "    # Initialize the Azure AI Search tool with the required parameters\n",
    "    ai_search = AzureAISearchTool(\n",
    "        index_connection_id=os.environ[\"AZURE_AI_CONNECTION_ID\"],  # Connection ID for the Azure AI Search index\n",
    "        index_name=\"insurance-documents-index\",  # Name of the search index\n",
    "        query_type=AzureAISearchQueryType.SIMPLE,  # Query type (e.g., SIMPLE, FULL)\n",
    "        top_k=3,  # Number of top results to retrieve\n",
    "        filter=\"\",  # Optional filter for search results\n",
    "    )\n",
    "\n",
    "    # Create an agent with the specified model, name, instructions, and tools\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=model_deployment_name,  # Model deployment name\n",
    "        name=\"policy-checker\",  # Name of the agent\n",
    "        instructions=\n",
    "        \"\"\"\"\n",
    "        You are an expert Insurance Policy Checker Agent specialized in analyzing auto insurance policies and validating claim coverage. Your primary responsibilities include:\n",
    "\n",
    "        **Core Functions:**\n",
    "        - Analyze insurance policy documents to determine coverage details\n",
    "        - Validate if specific claims are covered under policy terms\n",
    "        - Explain policy limits, deductibles, and exclusions\n",
    "        - Identify coverage gaps or restrictions\n",
    "        - Provide clear explanations of policy benefits\n",
    "\n",
    "        **Policy Types You Handle:**\n",
    "        - Commercial Auto Policies\n",
    "        - Comprehensive Auto Policies  \n",
    "        - High Value Vehicle Policies\n",
    "        - Liability Only Policies\n",
    "        - Motorcycle Policies\n",
    "\n",
    "        **Analysis Guidelines:**\n",
    "        1. Always reference specific policy sections when providing coverage determinations\n",
    "        2. Clearly state coverage limits, deductibles, and any applicable restrictions\n",
    "        3. Identify any exclusions that may apply to the claim\n",
    "        4. Be precise about effective dates and policy periods\n",
    "        5. Flag any discrepancies between claim details and policy terms\n",
    "\n",
    "        **Response Format:**\n",
    "        - Start with a clear coverage determination (COVERED/NOT COVERED/PARTIAL COVERAGE)\n",
    "        - Provide the specific policy section reference\n",
    "        - Explain coverage limits and deductibles\n",
    "        - List any relevant exclusions or conditions\n",
    "        - Suggest next steps if coverage issues exist\n",
    "        - Everything in a clear, concise manner in one paragraph.\n",
    "\n",
    "        **Tone:** Professional, accurate, and helpful. Always be thorough in your analysis while remaining clear and concise.\n",
    "\n",
    "        When you cannot find specific information in the policy documents, clearly state what information is missing and what additional documentation would be needed.\n",
    "        \"\"\",  # Instructions for the agent\n",
    "        tools=ai_search.definitions,  # Tools available to the agent\n",
    "        tool_resources=ai_search.resources,  # Resources for the tools\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "    # Create a thread for communication with the agent\n",
    "    thread = project_client.agents.threads.create()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Send a message to the thread\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,  # ID of the thread\n",
    "        role=MessageRole.USER,  # Role of the message sender (e.g., user)\n",
    "        content=\"What motorcycle coverage options are available?\",  # Message content\n",
    "    )\n",
    "    print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "    # Create and process an agent run in the thread using the tools\n",
    "    run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        # Log the error if the run fails\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Fetch and log all messages from the thread\n",
    "    messages = project_client.agents.messages.list(thread_id=thread.id, order=ListSortOrder.ASCENDING)\n",
    "    for message in messages:\n",
    "        # Only show the agent's response, not the user's question\n",
    "        if message.role == MessageRole.AGENT:\n",
    "            if message.content and len(message.content) > 0:\n",
    "                content_item = message.content[0]\n",
    "                if content_item.get('type') == 'text' and 'text' in content_item:\n",
    "                    agent_response = content_item['text']['value']\n",
    "                    print(agent_response)\n",
    "                    break  # Only show the first agent response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Azure Services Setup\n",
    "\n",
    "The next cell initializes Azure service clients by creating a BlobServiceClient for Azure Storage and an AzureOpenAI client for GPT-4o processing, with error handling to ensure both connections are established successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure clients initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure OpenAI client\n",
    "        openai_client = AzureOpenAI(\n",
    "            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "            api_version=Config.AZURE_OPENAI_API_VERSION\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Azure clients initialized successfully!\")\n",
    "        return blob_service_client, openai_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing clients: {e}\")\n",
    "        return None, None\n",
    "\n",
    "blob_service_client, openai_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates and tests Azure Blob Storage containers with enhanced error handling, checking connections, listing existing containers, and attempting to create the required containers (policies, claims, statements, processed-documents) while providing detailed diagnostics for any failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running enhanced container creation...\n",
      "üîç Testing storage account connection...\n",
      "‚úÖ Connected to storage account successfully\n",
      "   Account kind: StorageV2\n",
      "   SKU name: Standard_LRS\n",
      "\n",
      "üîç Checking existing containers...\n",
      "‚úÖ Found 4 existing containers: ['claims', 'policies', 'processed-documents', 'statements']\n",
      "‚ÑπÔ∏è Container 'policies' already exists\n",
      "‚ÑπÔ∏è Container 'claims' already exists\n",
      "‚ÑπÔ∏è Container 'statements' already exists\n",
      "‚ÑπÔ∏è Container 'processed-documents' already exists\n",
      "\n",
      "üìä Container Creation Summary:\n",
      "   Successful: 4 - ['policies', 'claims', 'statements', 'processed-documents']\n",
      "   Failed: 0 - []\n"
     ]
    }
   ],
   "source": [
    "# Enhanced container creation with multiple authentication methods and diagnostics\n",
    "def create_containers_enhanced(blob_service_client):\n",
    "    \"\"\"Create blob storage containers with enhanced error handling and diagnostics\"\"\"\n",
    "    \n",
    "    # First, test the connection\n",
    "    try:\n",
    "        print(\"üîç Testing storage account connection...\")\n",
    "        account_info = blob_service_client.get_account_information()\n",
    "        print(f\"‚úÖ Connected to storage account successfully\")\n",
    "        print(f\"   Account kind: {account_info.get('account_kind', 'Unknown')}\")\n",
    "        print(f\"   SKU name: {account_info.get('sku_name', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to storage account: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test listing existing containers\n",
    "    try:\n",
    "        print(\"\\nüîç Checking existing containers...\")\n",
    "        existing_containers = []\n",
    "        for container in blob_service_client.list_containers():\n",
    "            existing_containers.append(container.name)\n",
    "        print(f\"‚úÖ Found {len(existing_containers)} existing containers: {existing_containers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to list containers: {e}\")\n",
    "        print(\"   This might indicate insufficient permissions\")\n",
    "    \n",
    "    # Try to create containers\n",
    "    containers = [\n",
    "        Config.POLICIES_CONTAINER,\n",
    "        Config.CLAIMS_CONTAINER,\n",
    "        Config.STATEMENTS_CONTAINER,  # Added statements container\n",
    "        Config.PROCESSED_CONTAINER\n",
    "    ]\n",
    "    \n",
    "    created_containers = []\n",
    "    failed_containers = []\n",
    "    \n",
    "    for container_name in containers:\n",
    "        try:\n",
    "            # Check if container already exists first\n",
    "            container_client = blob_service_client.get_container_client(container_name)\n",
    "            \n",
    "            try:\n",
    "                # Try to get container properties (this will fail if it doesn't exist)\n",
    "                properties = container_client.get_container_properties()\n",
    "                print(f\"‚ÑπÔ∏è Container '{container_name}' already exists\")\n",
    "                created_containers.append(container_name)\n",
    "                continue\n",
    "            except Exception:\n",
    "                # Container doesn't exist, try to create it\n",
    "                pass\n",
    "            \n",
    "            # Create the container\n",
    "            print(f\"üî® Creating container '{container_name}'...\")\n",
    "            container_client.create_container()\n",
    "            print(f\"‚úÖ Container '{container_name}' created successfully\")\n",
    "            created_containers.append(container_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with container '{container_name}': {e}\")\n",
    "            failed_containers.append((container_name, str(e)))\n",
    "            \n",
    "            # Additional diagnostics for authorization errors\n",
    "            if \"AuthorizationFailure\" in str(e):\n",
    "                print(f\"   üîç Authorization issue detected for '{container_name}'\")\n",
    "                print(f\"   This could be due to:\")\n",
    "                print(f\"   - Storage account access keys disabled\")\n",
    "                print(f\"   - Network access restrictions\")\n",
    "                print(f\"   - Storage account permissions\")\n",
    "    \n",
    "    print(f\"\\nüìä Container Creation Summary:\")\n",
    "    print(f\"   Successful: {len(created_containers)} - {created_containers}\")\n",
    "    print(f\"   Failed: {len(failed_containers)} - {[name for name, _ in failed_containers]}\")\n",
    "    \n",
    "    return len(failed_containers) == 0\n",
    "\n",
    "if blob_service_client:\n",
    "    print(\"üöÄ Running enhanced container creation...\")\n",
    "    success = create_containers_enhanced(blob_service_client)\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\nüîÑ Primary method failed, trying alternatives...\")\n",
    "        \n",
    "        # Try alternative authentication\n",
    "        alt_client = try_alternative_authentication()\n",
    "        if alt_client:\n",
    "            blob_service_client = alt_client\n",
    "            success = create_containers_enhanced(blob_service_client)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Upload Functions\n",
    "The next cell creates a helpful DocumentUploader class that provides easy-to-use methods for uploading individual files or entire directories to Azure Blob Storage, complete with progress tracking and error handling to make document management seamless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document uploader initialized!\n"
     ]
    }
   ],
   "source": [
    "class DocumentUploader:\n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def upload_file(self, file_path: Path, container_name: str, blob_name: str = None) -> bool:\n",
    "        \"\"\"Upload a single file to blob storage\"\"\"\n",
    "        if blob_name is None:\n",
    "            blob_name = file_path.name\n",
    "            \n",
    "        try:\n",
    "            blob_client = self.blob_service_client.get_blob_client(\n",
    "                container=container_name, \n",
    "                blob=blob_name\n",
    "            )\n",
    "            \n",
    "            with open(file_path, 'rb') as data:\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "            print(f\"‚úÖ Uploaded: {file_path.name} ‚Üí {container_name}/{blob_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading {file_path.name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def upload_directory(self, directory_path: Path, container_name: str) -> Dict[str, bool]:\n",
    "        \"\"\"Upload all files from a directory to blob storage\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if not directory_path.exists():\n",
    "            print(f\"‚ùå Directory not found: {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        files = list(directory_path.glob('*'))\n",
    "        if not files:\n",
    "            print(f\"‚ÑπÔ∏è No files found in {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"üì§ Uploading {len(files)} files from {directory_path} to {container_name}...\")\n",
    "        \n",
    "        for file_path in tqdm(files, desc=\"Uploading files\"):\n",
    "            if file_path.is_file():\n",
    "                success = self.upload_file(file_path, container_name)\n",
    "                results[file_path.name] = success\n",
    "        \n",
    "        successful_uploads = sum(results.values())\n",
    "        print(f\"\\nüìä Upload Summary: {successful_uploads}/{len(results)} files uploaded successfully\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def list_blobs(self, container_name: str) -> List[str]:\n",
    "        \"\"\"List all blobs in a container\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "            blob_list = container_client.list_blobs()\n",
    "            return [blob.name for blob in blob_list]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error listing blobs in {container_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize uploader\n",
    "if blob_service_client:\n",
    "    uploader = DocumentUploader(blob_service_client)\n",
    "    print(\"‚úÖ Document uploader initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents to Blob Storage\n",
    "\n",
    "Separated by folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Uploading Policy Documents...\n",
      "==================================================\n",
      "üì§ Uploading 5 files from data\\policies to policies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: commercial_auto_policy.md ‚Üí policies/commercial_auto_policy.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: comprehensive_auto_policy.md ‚Üí policies/comprehensive_auto_policy.md\n",
      "‚úÖ Uploaded: high_value_vehicle_policy.md ‚Üí policies/high_value_vehicle_policy.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:00<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: liability_only_policy.md ‚Üí policies/liability_only_policy.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: motorcycle_policy.md ‚Üí policies/motorcycle_policy.md\n",
      "\n",
      "üìä Upload Summary: 5/5 files uploaded successfully\n",
      "\n",
      "üñºÔ∏è Uploading Claims Documents...\n",
      "==================================================\n",
      "üì§ Uploading 6 files from data\\claims to claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  17%|‚ñà‚ñã        | 1/6 [00:00<00:01,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash1.jpg ‚Üí claims/crash1.jpg\n",
      "‚úÖ Uploaded: crash2.jpg ‚Üí claims/crash2.jpg\n",
      "‚úÖ Uploaded: crash3.jpg ‚Üí claims/crash3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:00<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash4.jpeg ‚Üí claims/crash4.jpeg\n",
      "‚úÖ Uploaded: crash5.jpg ‚Üí claims/crash5.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: invoice.png ‚Üí claims/invoice.png\n",
      "\n",
      "üìä Upload Summary: 6/6 files uploaded successfully\n",
      "\n",
      "üìÑ Uploading Statements Documents...\n",
      "==================================================\n",
      "üì§ Uploading 5 files from data\\statements to statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash1.md ‚Üí statements/crash1.md\n",
      "‚úÖ Uploaded: crash2.md ‚Üí statements/crash2.md\n",
      "‚úÖ Uploaded: crash3.md ‚Üí statements/crash3.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash4.md ‚Üí statements/crash4.md\n",
      "‚úÖ Uploaded: crash5.md ‚Üí statements/crash5.md\n",
      "\n",
      "üìä Upload Summary: 5/5 files uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload policy documents\n",
    "print(\"üìÑ Uploading Policy Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "policy_results = uploader.upload_directory(Config.POLICIES_DIR, Config.POLICIES_CONTAINER)\n",
    "\n",
    "# Upload claims documents\n",
    "print(\"\\nüñºÔ∏è Uploading Claims Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "claims_results = uploader.upload_directory(Config.CLAIMS_DIR, Config.CLAIMS_CONTAINER)\n",
    "\n",
    "# Upload statements documents\n",
    "print(\"\\nüìÑ Uploading Statements Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "statements_results = uploader.upload_directory(Config.STATEMENTS_DIR, Config.STATEMENTS_CONTAINER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Processing with Azure OpenAI GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As of this moment we have created 3 containers that have the data that we will use to our use case. Awesome! Now, it's time to process the data. We are currently handling `.md`and `.png` files. For such, we will create a class called `DocumentProcessor` that will have 2 key functions:\n",
    "- **process_markdown_for_vectorization** - will process the markdown files as normal text files for vectorization\n",
    "- **generate_image_description_with_gpt4o** - will use the multimodal capabilities of GPT-4.1-mini to process our image and give us a description. Later on, this will be really important for fraud analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, openai_client, blob_service_client):\n",
    "        self.openai_client = openai_client\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_blob_content(self, container_name: str, blob_name: str) -> bytes:\n",
    "        \"\"\"Download blob content as bytes\"\"\"\n",
    "        blob_client = self.blob_service_client.get_blob_client(\n",
    "            container=container_name, \n",
    "            blob=blob_name\n",
    "        )\n",
    "        blob_data = blob_client.download_blob()\n",
    "        return blob_data.readall()\n",
    "    \n",
    "    def encode_image_to_base64(self, image_bytes: bytes) -> str:\n",
    "        \"\"\"Encode image bytes to base64 string\"\"\"\n",
    "        return base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    def process_markdown_for_vectorization(self, container_name: str, blob_name: str) -> Dict:\n",
    "        \"\"\"Process markdown file for direct vectorization (no GPT-4o processing)\"\"\"\n",
    "        try:\n",
    "            print(f\"üìÑ Preparing markdown for vectorization: {blob_name}...\")\n",
    "            \n",
    "            # Download and decode content\n",
    "            blob_content = self.get_blob_content(container_name, blob_name)\n",
    "            content = blob_content.decode('utf-8')\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"markdown\",\n",
    "                \"text_length\": len(content),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"processing_method\": \"direct_vectorization\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"text\": content,  # Original markdown content for vectorization\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"markdown\"}\n",
    "            }\n",
    "\n",
    "    def generate_image_description_with_gpt4o(self, container_name: str, blob_name: str) -> Dict:\n",
    "        try:\n",
    "            print(f\"üñºÔ∏è Generating description for image: {blob_name}...\")\n",
    "            \n",
    "            # Download image content\n",
    "            image_bytes = self.get_blob_content(container_name, blob_name)\n",
    "            base64_image = self.encode_image_to_base64(image_bytes)\n",
    "            \n",
    "            # Determine image format from file extension\n",
    "            file_extension = Path(blob_name).suffix.lower()\n",
    "            if file_extension == \".jpg\" or file_extension == \".jpeg\":\n",
    "                image_format = \"jpeg\"\n",
    "            elif file_extension == \".png\":\n",
    "                image_format = \"png\"\n",
    "            else:\n",
    "                image_format = \"jpeg\"  # default\n",
    "            \n",
    "            # Process with GPT-4.1-mini for description generation\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are an expert insurance claims analyst with advanced image analysis capabilities. \n",
    "                        Your task is to provide detailed, professional descriptions of insurance-related images, particularly vehicle damage and accident scenes.\n",
    "                        \n",
    "                        Focus on:\n",
    "                        - Type of vehicle and visible damage\n",
    "                        - Location and extent of damage (scratches, dents, broken parts, etc.)\n",
    "                        - Environmental context (road conditions, weather signs, location type)\n",
    "                        - Any visible people, other vehicles, or relevant objects\n",
    "                        - Overall severity assessment\n",
    "                        - Any safety concerns or hazards visible\n",
    "                        \n",
    "                        Provide clear, objective descriptions that would be useful for insurance claim processing and risk assessment.\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Please provide a detailed description of this insurance claim image. Focus on damage assessment, environmental factors, and any relevant details for insurance processing.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/{image_format};base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=4000,\n",
    "                temperature=0.3  # Slightly higher for more descriptive language\n",
    "            )\n",
    "            description = response.choices[0].message.content\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"image\",\n",
    "                \"image_format\": image_format,\n",
    "                \"image_size_bytes\": len(image_bytes),\n",
    "                \"description_length\": len(description),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"model_used\": Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                \"processing_type\": \"image_description\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"description\": description,  # Changed from \"text\" to \"description\"\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"image\"}\n",
    "            }\n",
    "\n",
    "    def process_all_documents(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Process documents: prepare markdown for vectorization, generate descriptions for images\"\"\"\n",
    "        results = {\n",
    "            \"policies\": [],\n",
    "            \"claims\": [],\n",
    "            \"statements\": []  # Added statements to results\n",
    "        }\n",
    "        \n",
    "        # Process policy documents (markdown files) - prepare for vectorization only\n",
    "        print(\"üìÑ Preparing Policy Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        policy_blobs = uploader.list_blobs(Config.POLICIES_CONTAINER)\n",
    "        for blob_name in tqdm(policy_blobs, desc=\"Preparing policies\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.POLICIES_CONTAINER, blob_name)\n",
    "                results[\"policies\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process statements documents (markdown files) - prepare for vectorization only\n",
    "        print(\"\\nüìÑ Preparing Statements Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        statements_blobs = uploader.list_blobs(Config.STATEMENTS_CONTAINER)\n",
    "        for blob_name in tqdm(statements_blobs, desc=\"Preparing statements\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.STATEMENTS_CONTAINER, blob_name)\n",
    "                results[\"statements\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process claims documents (images) - generate descriptions with GPT-4.1-mini \n",
    "        print(\"\\nüñºÔ∏è Generating Image Descriptions with GPT-4.1-mini ...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        claims_blobs = uploader.list_blobs(Config.CLAIMS_CONTAINER)\n",
    "        for blob_name in tqdm(claims_blobs, desc=\"Generating descriptions\"):\n",
    "            if blob_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                result = self.generate_image_description_with_gpt4o(Config.CLAIMS_CONTAINER, blob_name)\n",
    "                results[\"claims\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-image file: {blob_name}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_processed_results(self, results: Dict, output_file: str = \"processed_documents_for_vectorization.json\"):\n",
    "        \"\"\"Save processed results to JSON file and upload to blob storage\"\"\"\n",
    "        try:\n",
    "            # Save locally\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"üíæ Results saved locally: {output_file}\")\n",
    "            \n",
    "            # Upload to blob storage\n",
    "            success = uploader.upload_file(\n",
    "                Path(output_file), \n",
    "                Config.PROCESSED_CONTAINER, \n",
    "                output_file\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"‚òÅÔ∏è Results uploaded to blob storage: {Config.PROCESSED_CONTAINER}/{output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process All Documents with GPT-4.1-mini\n",
    "\n",
    "Now, let's seat and watch the magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processor initialized with GPT-4o!\n",
      "\n",
      "üöÄ Starting document processing with GPT-4o...\n",
      "============================================================\n",
      "üìÑ Preparing Policy Documents for Vectorization...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: commercial_auto_policy.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: comprehensive_auto_policy.md...\n",
      "üìÑ Preparing markdown for vectorization: high_value_vehicle_policy.md...\n",
      "üìÑ Preparing markdown for vectorization: liability_only_policy.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: motorcycle_policy.md...\n",
      "\n",
      "üìÑ Preparing Statements Documents for Vectorization...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing statements:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: crash1.md...\n",
      "üìÑ Preparing markdown for vectorization: crash2.md...\n",
      "üìÑ Preparing markdown for vectorization: crash3.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing statements: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: crash4.md...\n",
      "üìÑ Preparing markdown for vectorization: crash5.md...\n",
      "\n",
      "üñºÔ∏è Generating Image Descriptions with GPT-4.1-mini ...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating descriptions:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash1.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  17%|‚ñà‚ñã        | 1/6 [00:05<00:29,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash2.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:10<00:20,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash3.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:15<00:15,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash4.jpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:21<00:10,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash5.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:27<00:05,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: invoice.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:31<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Document processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor with GPT-4o\n",
    "if openai_client and blob_service_client:\n",
    "    processor = DocumentProcessor(openai_client, blob_service_client)\n",
    "    print(\"‚úÖ Document processor initialized with GPT-4o!\")\n",
    "    \n",
    "    # Process documents using GPT-4o\n",
    "    print(\"\\nüöÄ Starting document processing with GPT-4o...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processing_results = processor.process_all_documents()\n",
    "    \n",
    "    print(\"\\n‚úÖ Document processing completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize processor - missing clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Summary\n",
    "\n",
    "Perfect, now let's run the following code and we will be able to check locally the transcription of our files. Please do double check if they make sense. Special attention should be given to the information extracted from the images, as it should contain deeply detailed description of the crash in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved locally: processed_documents_for_vectorization.json\n",
      "‚úÖ Uploaded: processed_documents_for_vectorization.json ‚Üí processed-documents/processed_documents_for_vectorization.json\n",
      "‚òÅÔ∏è Results uploaded to blob storage: processed-documents/processed_documents_for_vectorization.json\n"
     ]
    }
   ],
   "source": [
    "# Save processing results\n",
    "processor.save_processed_results(processing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Cosmos our data!\n",
    "\n",
    "But first... if you inspected correctly you might have seen that we have indeed extracted data from our files, but it is not structured at all. We might as well do that! We will use the Claimant_ID on the top part of each submission (carefully processed by our teams) to create a database. And of course, to do that we will use... Generative AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "\n",
    "# Rename the model to something more appropriate\n",
    "class ClaimInfo(BaseModel):\n",
    "    claimant_id: str\n",
    "    policyholder_name: str\n",
    "    policyholder_address: str\n",
    "    policyholder_phone: str\n",
    "    policyholder_email: str\n",
    "    policy_number: str\n",
    "    vehicle_year_make_model: str\n",
    "    vehicle_color: str\n",
    "    vehicle_vin: str\n",
    "    vehicle_license_plate: str\n",
    "    incident_date: str\n",
    "    incident_time: str\n",
    "    incident_location: str\n",
    "    incident_description: str\n",
    "    damage_description: str\n",
    "    witness_name: str\n",
    "    witness_phone: str\n",
    "    police_department: str\n",
    "    police_report_number: str\n",
    "    repair_shop_name: str\n",
    "    repair_shop_address: str\n",
    "    attachments: str\n",
    "    claim_request: str\n",
    "    signature_name: str\n",
    "    signature_date: str\n",
    "\n",
    "\n",
    "def extract_structured_claim_info(text_content: str, claim_id: str) -> dict:\n",
    "    \"\"\"Extract structured information from claim text using Azure OpenAI structured outputs\"\"\"\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        \n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,  # Use your deployment name\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"\"\"You are an expert insurance claims processor. Extract structured information from crash statements and insurance claims. \n",
    "                    If any field is not available in the text, use \"N/A\" as the value. \n",
    "                    Be thorough and accurate in extracting all available information.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Extract the structured information from this crash statement for claim {claim_id}:\\n\\n{text_content}\"\n",
    "                },\n",
    "            ],\n",
    "            response_format=ClaimInfo,\n",
    "        )\n",
    "        \n",
    "        structured_data = completion.choices[0].message.parsed\n",
    "        print(f\"‚úÖ Extracted structured data for claim {claim_id}\")\n",
    "        return structured_data.model_dump()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting structured data for claim {claim_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting simplified crash report processing...\n",
      "üîç Extracting structured info for crash1 (Claim: CL001)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted structured data for claim CL001\n",
      "üîç Extracting structured info for crash2 (Claim: CL002)...\n",
      "‚úÖ Extracted structured data for claim CL002\n",
      "üîç Extracting structured info for crash3 (Claim: CL003)...\n",
      "‚úÖ Extracted structured data for claim CL003\n",
      "üîç Extracting structured info for crash4 (Claim: CL001)...\n",
      "‚úÖ Extracted structured data for claim CL001\n",
      "üîç Extracting structured info for crash5 (Claim: CL004)...\n",
      "‚úÖ Extracted structured data for claim CL004\n",
      "‚úÖ Created simplified report for Claim ID: CL001 (2 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL002 (1 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL003 (1 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL004 (1 crashes)\n",
      "\n",
      "üìä Created 4 simplified crash reports\n",
      "\n",
      "üìã Simplified Report Structure:\n",
      "  Claim CL001:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 2\n",
      "    - Policyholder: John Peterson, Policy: 314579824\n",
      "  Claim CL002:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Samantha Turner, Policy: 4421983201\n",
      "  Claim CL003:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Michael Rodriguez, Policy: 5931784520\n",
      "  Claim CL004:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Christopher J. Ryan, Policy: 1283947502\n",
      "üíæ Saving simplified crash reports...\n",
      "‚úÖ Saved simplified Claim CL001 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: John Peterson\n",
      "      policy_number: 314579824\n",
      "      incident_date: July 17, 2025\n",
      "      incident_location: Parking lot, 2325 Main Street, Springfield, OH 45503\n",
      "‚úÖ Saved simplified Claim CL002 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Samantha Turner\n",
      "      policy_number: 4421983201\n",
      "      incident_date: July 18, 2025\n",
      "      incident_location: 2100 Madison Avenue, Albany, NY 12208\n",
      "‚úÖ Saved simplified Claim CL003 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Michael Rodriguez\n",
      "      policy_number: 5931784520\n",
      "      incident_date: July 19, 2025\n",
      "      incident_location: 400 East 8th Street, Harrisburg, PA 17102\n",
      "‚úÖ Saved simplified Claim CL004 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Christopher J. Ryan\n",
      "      policy_number: 1283947502\n",
      "      incident_date: July 19, 2025\n",
      "      incident_location: Intersection of Karl Road & Bethel Road, Columbus, OH 43214\n",
      "üíæ Simplified crash reports saved to 'simplified_crash_reports.json'\n",
      "\n",
      "üìÑ Sample of simplified JSON structure:\n",
      "{\n",
      "  \"sample_claim\": {\n",
      "    \"claim_id\": \"CL001\",\n",
      "    \"structured_claim_info\": {\n",
      "      \"policyholder_name\": \"John Peterson\",\n",
      "      \"policy_number\": \"314579824\",\n",
      "      \"incident_date\": \"July 17, 2025\",\n",
      "      \"...\": \"all other structured fields\"\n",
      "    },\n",
      "    \"image_descriptions\": [\n",
      "      {\n",
      "        \"crash_number\": \"crash1\",\n",
      "        \"image_file\": \"crash1.jpg\",\n",
      "        \"description\": \"Detailed image description...\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def process_crash_reports_simplified():\n",
    "    \"\"\"Process JSON file and create simplified crash reports with only structured info and image descriptions\"\"\"\n",
    "    \n",
    "    # Load the processed JSON file\n",
    "    with open('processed_documents_for_vectorization.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    statements_lookup = {item['metadata']['file_name']: item for item in data['statements'] if item['success']}\n",
    "    claims_lookup = {item['metadata']['file_name']: item for item in data['claims'] if item['success']}\n",
    "    \n",
    "    # Define crash to claim ID mapping\n",
    "    crash_to_claim_mapping = {\n",
    "        'crash1': 'CL001',\n",
    "        'crash2': 'CL002', \n",
    "        'crash3': 'CL003',\n",
    "        'crash4': 'CL001',  # Same claim as crash1\n",
    "        'crash5': 'CL004'\n",
    "    }\n",
    "    \n",
    "    # Group by claim ID to handle multiple crashes per claim\n",
    "    claims_data = {}\n",
    "    \n",
    "    for crash_num, claim_id in crash_to_claim_mapping.items():\n",
    "        statement_file = f\"{crash_num}.md\"\n",
    "        image_files = [f\"{crash_num}.jpg\", f\"{crash_num}.jpeg\"]\n",
    "        \n",
    "        # Find matching image file\n",
    "        image_file = None\n",
    "        for img in image_files:\n",
    "            if img in claims_lookup:\n",
    "                image_file = img\n",
    "                break\n",
    "        \n",
    "        if statement_file in statements_lookup and image_file:\n",
    "            statement_data = statements_lookup[statement_file]\n",
    "            image_data = claims_lookup[image_file]\n",
    "            \n",
    "            if claim_id not in claims_data:\n",
    "                claims_data[claim_id] = {\n",
    "                    \"structured_info\": [],\n",
    "                    \"image_descriptions\": []\n",
    "                }\n",
    "            \n",
    "            # Extract structured information from the statement text\n",
    "            print(f\"üîç Extracting structured info for {crash_num} (Claim: {claim_id})...\")\n",
    "            structured_info = extract_structured_claim_info(statement_data['text'], claim_id)\n",
    "            \n",
    "            if structured_info:\n",
    "                claims_data[claim_id][\"structured_info\"].append({\n",
    "                    \"crash_number\": crash_num,\n",
    "                    \"structured_data\": structured_info\n",
    "                })\n",
    "            \n",
    "            # Add image description\n",
    "            claims_data[claim_id][\"image_descriptions\"].append({\n",
    "                \"crash_number\": crash_num,\n",
    "                \"image_file\": image_file,\n",
    "                \"description\": image_data['description']\n",
    "            })\n",
    "    \n",
    "    # Create simplified crash reports with only structured info and image descriptions\n",
    "    simplified_reports = []\n",
    "    for claim_id, claim_data in claims_data.items():\n",
    "        # Combine structured info from all crashes in this claim\n",
    "        combined_structured_info = {}\n",
    "        if claim_data[\"structured_info\"]:\n",
    "            # Use the first crash's structured info as base\n",
    "            combined_structured_info = claim_data[\"structured_info\"][0][\"structured_data\"].copy()\n",
    "            \n",
    "            # For claims with multiple crashes, add them as additional crashes\n",
    "            if len(claim_data[\"structured_info\"]) > 1:\n",
    "                combined_structured_info[\"additional_crashes\"] = []\n",
    "                for i in range(1, len(claim_data[\"structured_info\"])):\n",
    "                    additional_crash = {\n",
    "                        \"crash_number\": claim_data[\"structured_info\"][i][\"crash_number\"],\n",
    "                        \"structured_data\": claim_data[\"structured_info\"][i][\"structured_data\"]\n",
    "                    }\n",
    "                    combined_structured_info[\"additional_crashes\"].append(additional_crash)\n",
    "        \n",
    "        simplified_report = {\n",
    "            \"claim_id\": claim_id,\n",
    "            \"structured_claim_info\": combined_structured_info,\n",
    "            \"image_descriptions\": claim_data[\"image_descriptions\"]\n",
    "        }\n",
    "        \n",
    "        simplified_reports.append(simplified_report)\n",
    "        crashes_count = len(claim_data[\"structured_info\"])\n",
    "        print(f\"‚úÖ Created simplified report for Claim ID: {claim_id} ({crashes_count} crashes)\")\n",
    "    \n",
    "    return simplified_reports\n",
    "\n",
    "def save_simplified_cosmos_db(simplified_reports):\n",
    "    \"\"\"Save simplified reports to Cosmos DB\"\"\"\n",
    "    \n",
    "    # Initialize Cosmos client\n",
    "    client = CosmosClient(Config.COSMOS_ENDPOINT, Config.COSMOS_KEY)\n",
    "    database = client.create_database_if_not_exists(id=Config.COSMOS_DATABASE)\n",
    "    \n",
    "    # Update partition key to use claim_id\n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=Config.COSMOS_CONTAINER,\n",
    "        partition_key=PartitionKey(path=\"/claim_id\")\n",
    "    )\n",
    "    \n",
    "    # Save simplified reports\n",
    "    print(\"üíæ Saving simplified crash reports...\")\n",
    "    for report in simplified_reports:\n",
    "        try:\n",
    "            # Add required id field for Cosmos DB\n",
    "            report_with_id = report.copy()\n",
    "            report_with_id[\"id\"] = report[\"claim_id\"]\n",
    "            \n",
    "            container.upsert_item(body=report_with_id)\n",
    "            print(f\"‚úÖ Saved simplified Claim {report['claim_id']} to Cosmos DB\")\n",
    "            \n",
    "            # Print sample of structured info for verification\n",
    "            if report.get(\"structured_claim_info\"):\n",
    "                sample_fields = [\"policyholder_name\", \"policy_number\", \"incident_date\", \"incident_location\"]\n",
    "                print(f\"   üìã Sample structured data:\")\n",
    "                for field in sample_fields:\n",
    "                    value = report[\"structured_claim_info\"].get(field, \"N/A\")\n",
    "                    print(f\"      {field}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving {report['claim_id']}: {e}\")\n",
    "\n",
    "# Execute the simplified processing\n",
    "print(\"üöÄ Starting simplified crash report processing...\")\n",
    "simplified_reports = process_crash_reports_simplified()\n",
    "print(f\"\\nüìä Created {len(simplified_reports)} simplified crash reports\")\n",
    "\n",
    "# Display the simplified structure\n",
    "print(\"\\nüìã Simplified Report Structure:\")\n",
    "for report in simplified_reports:\n",
    "    print(f\"  Claim {report['claim_id']}:\")\n",
    "    print(f\"    - Structured Info: {'‚úÖ' if report.get('structured_claim_info') else '‚ùå'}\")\n",
    "    print(f\"    - Image Descriptions: {len(report.get('image_descriptions', []))}\")\n",
    "    \n",
    "    # Show sample structured data\n",
    "    if report.get(\"structured_claim_info\"):\n",
    "        policyholder = report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\")\n",
    "        policy_num = report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\")\n",
    "        print(f\"    - Policyholder: {policyholder}, Policy: {policy_num}\")\n",
    "\n",
    "# Save to Cosmos DB\n",
    "save_simplified_cosmos_db(simplified_reports)\n",
    "\n",
    "# Save the simplified reports locally\n",
    "with open('simplified_crash_reports.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_reports, f, indent=2, ensure_ascii=False)\n",
    "print(f\"üíæ Simplified crash reports saved to 'simplified_crash_reports.json'\")\n",
    "\n",
    "# Show a sample of what the final JSON looks like\n",
    "print(\"\\nüìÑ Sample of simplified JSON structure:\")\n",
    "if simplified_reports:\n",
    "    sample_report = simplified_reports[0]\n",
    "    print(json.dumps({\n",
    "        \"sample_claim\": {\n",
    "            \"claim_id\": sample_report[\"claim_id\"],\n",
    "            \"structured_claim_info\": {\n",
    "                \"policyholder_name\": sample_report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\"),\n",
    "                \"policy_number\": sample_report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\"),\n",
    "                \"incident_date\": sample_report[\"structured_claim_info\"].get(\"incident_date\", \"N/A\"),\n",
    "                \"...\": \"all other structured fields\"\n",
    "            },\n",
    "            \"image_descriptions\": [\n",
    "                {\n",
    "                    \"crash_number\": sample_report[\"image_descriptions\"][0][\"crash_number\"],\n",
    "                    \"image_file\": sample_report[\"image_descriptions\"][0][\"image_file\"],\n",
    "                    \"description\": \"Detailed image description...\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
