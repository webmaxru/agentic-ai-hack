{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Azure OpenAI Multimodal Model\n",
    "\n",
    "This notebook demonstrates how to process insurance documents using Azure OpenAI's multimodal GPT-4o model, including both text and image analysis. The workflow includes:\n",
    "\n",
    "1. **Upload Data to Azure Blob Storage**: Upload insurance policy documents (.md files), claim statements (.md files), and crash images (.jpg/.png files) to Azure Blob Storage containers for organized processing.\n",
    "\n",
    "2. **Process Documents with Azure OpenAI GPT-4-1-mini**\n",
    "   - **Text Processing**: Prepare markdown files containing insurance policies and claim statements for vectorization\n",
    "   - **Image Analysis**: Use GPT-4o's vision capabilities to generate detailed descriptions of crash scene images, analyzing vehicle damage, environmental conditions, and relevant details for insurance claim processing\n",
    "\n",
    "3. **Extract Structured Information**: Utilize Azure OpenAI's structured output capabilities to extract key information from claim statements into structured JSON format, including policyholder details, incident information, vehicle data, and witness information.\n",
    "\n",
    "4. **Store in Azure Cosmos DB**: Save the processed and structured claim information to Azure Cosmos DB for easy retrieval and analysis by insurance agents and automated systems.\n",
    "\n",
    "5. **Retrieve Information in JSON Format**: Generate comprehensive JSON outputs containing both structured claim data and detailed image descriptions, ready for downstream processing like vectorization and RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "Automating document processing is crucial for improving efficiency and accuracy in handling large volumes of data. By leveraging Azure's cloud services, organizations can streamline their workflows, reduce manual errors, and gain valuable insights from their documents. This approach not only saves time and resources but also enhances data accessibility and decision-making capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's start with handling the import of our libraries and load the `.env` variables that we have saved in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.aio import AIProjectClient as AsyncAIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import MessageRole, ListSortOrder, AzureAISearchTool, AzureAISearchQueryType\n",
    "# OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell initializes Azure service clients - it creates connections to both Azure Blob Storage and Azure OpenAI services using the configuration variables from your .env file. The initialize_clients() function sets up the BlobServiceClient for file storage operations and the AzureOpenAI client for GPT-4o processing, with error handling to ensure both services are properly connected before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    AZURE_STORAGE_ACCOUNT_NAME = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')\n",
    "    AZURE_STORAGE_ACCOUNT_KEY = os.getenv('AZURE_STORAGE_ACCOUNT_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1-mini')\n",
    "        \n",
    "    # Cosmos DB configuration\n",
    "    COSMOS_ENDPOINT = os.getenv('COSMOS_ENDPOINT')\n",
    "    COSMOS_KEY = os.getenv('COSMOS_KEY')\n",
    "    COSMOS_DATABASE = 'insurance_claims'\n",
    "    COSMOS_CONTAINER = 'crash_reports'\n",
    "    \n",
    "    # Container names\n",
    "    POLICIES_CONTAINER = 'policies'\n",
    "    CLAIMS_CONTAINER = 'claims'\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    STATEMENTS_CONTAINER = 'statements'\n",
    "    \n",
    "    # Local data paths\n",
    "    DATA_DIR = Path('data')\n",
    "    POLICIES_DIR = DATA_DIR / 'policies'\n",
    "    CLAIMS_DIR = DATA_DIR / 'claims'\n",
    "    STATEMENTS_DIR = DATA_DIR / 'statements'\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"âŒ Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables - please add these to your .env file:\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "    print(f\"ğŸ“ Policies directory: {Config.POLICIES_DIR}\")\n",
    "    print(f\"ğŸ“ Statements directory: {Config.STATEMENTS_DIR}\")\n",
    "    print(f\"ğŸ“ Claims directory: {Config.CLAIMS_DIR}\")\n",
    "    print(f\"ğŸ¤– OpenAI Deployment: {Config.AZURE_OPENAI_DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve endpoint and model deployment name from environment variables\n",
    "project_endpoint = os.getenv(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_deployment_name = \"gpt-4.1-mini\" \n",
    "\n",
    "# Initialize the AIProjectClient with the endpoint and credentials\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(exclude_interactive_browser_credential=False),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "with project_client:\n",
    "    # Initialize the Azure AI Search tool with the required parameters\n",
    "    ai_search = AzureAISearchTool(\n",
    "        index_connection_id=os.environ[\"AZURE_AI_CONNECTION_ID\"],  # Connection ID for the Azure AI Search index\n",
    "        index_name=\"insurance-documents-index\",  # Name of the search index\n",
    "        query_type=AzureAISearchQueryType.SIMPLE,  # Query type (e.g., SIMPLE, FULL)\n",
    "        top_k=3,  # Number of top results to retrieve\n",
    "        filter=\"\",  # Optional filter for search results\n",
    "    )\n",
    "\n",
    "    # Create an agent with the specified model, name, instructions, and tools\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=model_deployment_name,  # Model deployment name\n",
    "        name=\"policy-checker\",  # Name of the agent\n",
    "        instructions=\n",
    "        \"\"\"\"\n",
    "        You are an expert Insurance Policy Checker Agent specialized in analyzing auto insurance policies and validating claim coverage. Your primary responsibilities include:\n",
    "\n",
    "        **Core Functions:**\n",
    "        - Analyze insurance policy documents to determine coverage details\n",
    "        - Validate if specific claims are covered under policy terms\n",
    "        - Explain policy limits, deductibles, and exclusions\n",
    "        - Identify coverage gaps or restrictions\n",
    "        - Provide clear explanations of policy benefits\n",
    "\n",
    "        **Policy Types You Handle:**\n",
    "        - Commercial Auto Policies\n",
    "        - Comprehensive Auto Policies  \n",
    "        - High Value Vehicle Policies\n",
    "        - Liability Only Policies\n",
    "        - Motorcycle Policies\n",
    "\n",
    "        **Analysis Guidelines:**\n",
    "        1. Always reference specific policy sections when providing coverage determinations\n",
    "        2. Clearly state coverage limits, deductibles, and any applicable restrictions\n",
    "        3. Identify any exclusions that may apply to the claim\n",
    "        4. Be precise about effective dates and policy periods\n",
    "        5. Flag any discrepancies between claim details and policy terms\n",
    "\n",
    "        **Response Format:**\n",
    "        - Start with a clear coverage determination (COVERED/NOT COVERED/PARTIAL COVERAGE)\n",
    "        - Provide the specific policy section reference\n",
    "        - Explain coverage limits and deductibles\n",
    "        - List any relevant exclusions or conditions\n",
    "        - Suggest next steps if coverage issues exist\n",
    "        - Everything in a clear, concise manner in one paragraph.\n",
    "\n",
    "        **Tone:** Professional, accurate, and helpful. Always be thorough in your analysis while remaining clear and concise.\n",
    "\n",
    "        When you cannot find specific information in the policy documents, clearly state what information is missing and what additional documentation would be needed.\n",
    "        \"\"\",  # Instructions for the agent\n",
    "        tools=ai_search.definitions,  # Tools available to the agent\n",
    "        tool_resources=ai_search.resources,  # Resources for the tools\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "    # Create a thread for communication with the agent\n",
    "    thread = project_client.agents.threads.create()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Send a message to the thread\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,  # ID of the thread\n",
    "        role=MessageRole.USER,  # Role of the message sender (e.g., user)\n",
    "        content=\"What motorcycle coverage options are available?\",  # Message content\n",
    "    )\n",
    "    print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "    # Create and process an agent run in the thread using the tools\n",
    "    run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        # Log the error if the run fails\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Fetch and log all messages from the thread\n",
    "    messages = project_client.agents.messages.list(thread_id=thread.id, order=ListSortOrder.ASCENDING)\n",
    "    for message in messages:\n",
    "        # Only show the agent's response, not the user's question\n",
    "        if message.role == MessageRole.AGENT:\n",
    "            if message.content and len(message.content) > 0:\n",
    "                content_item = message.content[0]\n",
    "                if content_item.get('type') == 'text' and 'text' in content_item:\n",
    "                    agent_response = content_item['text']['value']\n",
    "                    print(agent_response)\n",
    "                    break  # Only show the first agent response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Azure Services Setup\n",
    "\n",
    "The next cell initializes Azure service clients by creating a BlobServiceClient for Azure Storage and an AzureOpenAI client for GPT-4o processing, with error handling to ensure both connections are established successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure OpenAI client\n",
    "        openai_client = AzureOpenAI(\n",
    "            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "            api_version=Config.AZURE_OPENAI_API_VERSION\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Azure clients initialized successfully!\")\n",
    "        return blob_service_client, openai_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error initializing clients: {e}\")\n",
    "        return None, None\n",
    "\n",
    "blob_service_client, openai_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates and tests Azure Blob Storage containers with enhanced error handling, checking connections, listing existing containers, and attempting to create the required containers (policies, claims, statements, processed-documents) while providing detailed diagnostics for any failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced container creation with multiple authentication methods and diagnostics\n",
    "def create_containers_enhanced(blob_service_client):\n",
    "    \"\"\"Create blob storage containers with enhanced error handling and diagnostics\"\"\"\n",
    "    \n",
    "    # First, test the connection\n",
    "    try:\n",
    "        print(\"ğŸ” Testing storage account connection...\")\n",
    "        account_info = blob_service_client.get_account_information()\n",
    "        print(f\"âœ… Connected to storage account successfully\")\n",
    "        print(f\"   Account kind: {account_info.get('account_kind', 'Unknown')}\")\n",
    "        print(f\"   SKU name: {account_info.get('sku_name', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to connect to storage account: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test listing existing containers\n",
    "    try:\n",
    "        print(\"\\nğŸ” Checking existing containers...\")\n",
    "        existing_containers = []\n",
    "        for container in blob_service_client.list_containers():\n",
    "            existing_containers.append(container.name)\n",
    "        print(f\"âœ… Found {len(existing_containers)} existing containers: {existing_containers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to list containers: {e}\")\n",
    "        print(\"   This might indicate insufficient permissions\")\n",
    "    \n",
    "    # Try to create containers\n",
    "    containers = [\n",
    "        Config.POLICIES_CONTAINER,\n",
    "        Config.CLAIMS_CONTAINER,\n",
    "        Config.STATEMENTS_CONTAINER,  # Added statements container\n",
    "        Config.PROCESSED_CONTAINER\n",
    "    ]\n",
    "    \n",
    "    created_containers = []\n",
    "    failed_containers = []\n",
    "    \n",
    "    for container_name in containers:\n",
    "        try:\n",
    "            # Check if container already exists first\n",
    "            container_client = blob_service_client.get_container_client(container_name)\n",
    "            \n",
    "            try:\n",
    "                # Try to get container properties (this will fail if it doesn't exist)\n",
    "                properties = container_client.get_container_properties()\n",
    "                print(f\"â„¹ï¸ Container '{container_name}' already exists\")\n",
    "                created_containers.append(container_name)\n",
    "                continue\n",
    "            except Exception:\n",
    "                # Container doesn't exist, try to create it\n",
    "                pass\n",
    "            \n",
    "            # Create the container\n",
    "            print(f\"ğŸ”¨ Creating container '{container_name}'...\")\n",
    "            container_client.create_container()\n",
    "            print(f\"âœ… Container '{container_name}' created successfully\")\n",
    "            created_containers.append(container_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with container '{container_name}': {e}\")\n",
    "            failed_containers.append((container_name, str(e)))\n",
    "            \n",
    "            # Additional diagnostics for authorization errors\n",
    "            if \"AuthorizationFailure\" in str(e):\n",
    "                print(f\"   ğŸ” Authorization issue detected for '{container_name}'\")\n",
    "                print(f\"   This could be due to:\")\n",
    "                print(f\"   - Storage account access keys disabled\")\n",
    "                print(f\"   - Network access restrictions\")\n",
    "                print(f\"   - Storage account permissions\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Container Creation Summary:\")\n",
    "    print(f\"   Successful: {len(created_containers)} - {created_containers}\")\n",
    "    print(f\"   Failed: {len(failed_containers)} - {[name for name, _ in failed_containers]}\")\n",
    "    \n",
    "    return len(failed_containers) == 0\n",
    "\n",
    "if blob_service_client:\n",
    "    print(\"ğŸš€ Running enhanced container creation...\")\n",
    "    success = create_containers_enhanced(blob_service_client)\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\nğŸ”„ Primary method failed, trying alternatives...\")\n",
    "        \n",
    "        # Try alternative authentication\n",
    "        alt_client = try_alternative_authentication()\n",
    "        if alt_client:\n",
    "            blob_service_client = alt_client\n",
    "            success = create_containers_enhanced(blob_service_client)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Upload Functions\n",
    "The next cell creates a helpful DocumentUploader class that provides easy-to-use methods for uploading individual files or entire directories to Azure Blob Storage, complete with progress tracking and error handling to make document management seamless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentUploader:\n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def upload_file(self, file_path: Path, container_name: str, blob_name: str = None) -> bool:\n",
    "        \"\"\"Upload a single file to blob storage\"\"\"\n",
    "        if blob_name is None:\n",
    "            blob_name = file_path.name\n",
    "            \n",
    "        try:\n",
    "            blob_client = self.blob_service_client.get_blob_client(\n",
    "                container=container_name, \n",
    "                blob=blob_name\n",
    "            )\n",
    "            \n",
    "            with open(file_path, 'rb') as data:\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "            print(f\"âœ… Uploaded: {file_path.name} â†’ {container_name}/{blob_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error uploading {file_path.name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def upload_directory(self, directory_path: Path, container_name: str) -> Dict[str, bool]:\n",
    "        \"\"\"Upload all files from a directory to blob storage\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if not directory_path.exists():\n",
    "            print(f\"âŒ Directory not found: {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        files = list(directory_path.glob('*'))\n",
    "        if not files:\n",
    "            print(f\"â„¹ï¸ No files found in {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"ğŸ“¤ Uploading {len(files)} files from {directory_path} to {container_name}...\")\n",
    "        \n",
    "        for file_path in tqdm(files, desc=\"Uploading files\"):\n",
    "            if file_path.is_file():\n",
    "                success = self.upload_file(file_path, container_name)\n",
    "                results[file_path.name] = success\n",
    "        \n",
    "        successful_uploads = sum(results.values())\n",
    "        print(f\"\\nğŸ“Š Upload Summary: {successful_uploads}/{len(results)} files uploaded successfully\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def list_blobs(self, container_name: str) -> List[str]:\n",
    "        \"\"\"List all blobs in a container\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "            blob_list = container_client.list_blobs()\n",
    "            return [blob.name for blob in blob_list]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error listing blobs in {container_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize uploader\n",
    "if blob_service_client:\n",
    "    uploader = DocumentUploader(blob_service_client)\n",
    "    print(\"âœ… Document uploader initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents to Blob Storage\n",
    "\n",
    "Separated by folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload policy documents\n",
    "print(\"ğŸ“„ Uploading Policy Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "policy_results = uploader.upload_directory(Config.POLICIES_DIR, Config.POLICIES_CONTAINER)\n",
    "\n",
    "# Upload claims documents\n",
    "print(\"\\nğŸ–¼ï¸ Uploading Claims Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "claims_results = uploader.upload_directory(Config.CLAIMS_DIR, Config.CLAIMS_CONTAINER)\n",
    "\n",
    "# Upload statements documents\n",
    "print(\"\\nğŸ“„ Uploading Statements Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "statements_results = uploader.upload_directory(Config.STATEMENTS_DIR, Config.STATEMENTS_CONTAINER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Processing with Azure OpenAI GPT-4-1-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As of this moment we have created 3 containers that have the data that we will use to our use case. Awesome! Now, it's time to process the data. We are currently handling `.md`and `.png` files. For such, we will create a class called `DocumentProcessor` that will have 2 key functions:\n",
    "- **process_markdown_for_vectorization** - will process the markdown files as normal text files for vectorization\n",
    "- **generate_image_description_with_gpt4o** - will use the multimodal capabilities of GPT-4.1-mini to process our image and give us a description. Later on, this will be really important for fraud analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, openai_client, blob_service_client):\n",
    "        self.openai_client = openai_client\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_blob_content(self, container_name: str, blob_name: str) -> bytes:\n",
    "        \"\"\"Download blob content as bytes\"\"\"\n",
    "        blob_client = self.blob_service_client.get_blob_client(\n",
    "            container=container_name, \n",
    "            blob=blob_name\n",
    "        )\n",
    "        blob_data = blob_client.download_blob()\n",
    "        return blob_data.readall()\n",
    "    \n",
    "    def encode_image_to_base64(self, image_bytes: bytes) -> str:\n",
    "        \"\"\"Encode image bytes to base64 string\"\"\"\n",
    "        return base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    def process_markdown_for_vectorization(self, container_name: str, blob_name: str) -> Dict:\n",
    "        \"\"\"Process markdown file for direct vectorization (no GPT-4o processing)\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ“„ Preparing markdown for vectorization: {blob_name}...\")\n",
    "            \n",
    "            # Download and decode content\n",
    "            blob_content = self.get_blob_content(container_name, blob_name)\n",
    "            content = blob_content.decode('utf-8')\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"markdown\",\n",
    "                \"text_length\": len(content),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"processing_method\": \"direct_vectorization\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"text\": content,  # Original markdown content for vectorization\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"markdown\"}\n",
    "            }\n",
    "\n",
    "    def generate_image_description_with_gpt4o(self, container_name: str, blob_name: str) -> Dict:\n",
    "        try:\n",
    "            print(f\"ğŸ–¼ï¸ Generating description for image: {blob_name}...\")\n",
    "            \n",
    "            # Download image content\n",
    "            image_bytes = self.get_blob_content(container_name, blob_name)\n",
    "            base64_image = self.encode_image_to_base64(image_bytes)\n",
    "            \n",
    "            # Determine image format from file extension\n",
    "            file_extension = Path(blob_name).suffix.lower()\n",
    "            if file_extension == \".jpg\" or file_extension == \".jpeg\":\n",
    "                image_format = \"jpeg\"\n",
    "            elif file_extension == \".png\":\n",
    "                image_format = \"png\"\n",
    "            else:\n",
    "                image_format = \"jpeg\"  # default\n",
    "            \n",
    "            # Process with GPT-4.1-mini for description generation\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are an expert insurance claims analyst with advanced image analysis capabilities. \n",
    "                        Your task is to provide detailed, professional descriptions of insurance-related images, particularly vehicle damage and accident scenes.\n",
    "                        \n",
    "                        Focus on:\n",
    "                        - Type of vehicle and visible damage\n",
    "                        - Location and extent of damage (scratches, dents, broken parts, etc.)\n",
    "                        - Environmental context (road conditions, weather signs, location type)\n",
    "                        - Any visible people, other vehicles, or relevant objects\n",
    "                        - Overall severity assessment\n",
    "                        - Any safety concerns or hazards visible\n",
    "                        \n",
    "                        Provide clear, objective descriptions that would be useful for insurance claim processing and risk assessment.\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Please provide a detailed description of this insurance claim image. Focus on damage assessment, environmental factors, and any relevant details for insurance processing.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/{image_format};base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=4000,\n",
    "                temperature=0.3  # Slightly higher for more descriptive language\n",
    "            )\n",
    "            description = response.choices[0].message.content\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"image\",\n",
    "                \"image_format\": image_format,\n",
    "                \"image_size_bytes\": len(image_bytes),\n",
    "                \"description_length\": len(description),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"model_used\": Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                \"processing_type\": \"image_description\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"description\": description,  # Changed from \"text\" to \"description\"\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"image\"}\n",
    "            }\n",
    "\n",
    "    def process_all_documents(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Process documents: prepare markdown for vectorization, generate descriptions for images\"\"\"\n",
    "        results = {\n",
    "            \"policies\": [],\n",
    "            \"claims\": [],\n",
    "            \"statements\": []  # Added statements to results\n",
    "        }\n",
    "        \n",
    "        # Process policy documents (markdown files) - prepare for vectorization only\n",
    "        print(\"ğŸ“„ Preparing Policy Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        policy_blobs = uploader.list_blobs(Config.POLICIES_CONTAINER)\n",
    "        for blob_name in tqdm(policy_blobs, desc=\"Preparing policies\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.POLICIES_CONTAINER, blob_name)\n",
    "                results[\"policies\"].append(result)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process statements documents (markdown files) - prepare for vectorization only\n",
    "        print(\"\\nğŸ“„ Preparing Statements Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        statements_blobs = uploader.list_blobs(Config.STATEMENTS_CONTAINER)\n",
    "        for blob_name in tqdm(statements_blobs, desc=\"Preparing statements\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.STATEMENTS_CONTAINER, blob_name)\n",
    "                results[\"statements\"].append(result)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process claims documents (images) - generate descriptions with GPT-4.1-mini \n",
    "        print(\"\\nğŸ–¼ï¸ Generating Image Descriptions with GPT-4.1-mini ...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        claims_blobs = uploader.list_blobs(Config.CLAIMS_CONTAINER)\n",
    "        for blob_name in tqdm(claims_blobs, desc=\"Generating descriptions\"):\n",
    "            if blob_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                result = self.generate_image_description_with_gpt4o(Config.CLAIMS_CONTAINER, blob_name)\n",
    "                results[\"claims\"].append(result)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Skipping non-image file: {blob_name}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_processed_results(self, results: Dict, output_file: str = \"processed_documents_for_vectorization.json\"):\n",
    "        \"\"\"Save processed results to JSON file and upload to blob storage\"\"\"\n",
    "        try:\n",
    "            # Save locally\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"ğŸ’¾ Results saved locally: {output_file}\")\n",
    "            \n",
    "            # Upload to blob storage\n",
    "            success = uploader.upload_file(\n",
    "                Path(output_file), \n",
    "                Config.PROCESSED_CONTAINER, \n",
    "                output_file\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"â˜ï¸ Results uploaded to blob storage: {Config.PROCESSED_CONTAINER}/{output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process All Documents with GPT-4.1-mini\n",
    "\n",
    "Now, let's seat and watch the magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor with GPT-4o\n",
    "if openai_client and blob_service_client:\n",
    "    processor = DocumentProcessor(openai_client, blob_service_client)\n",
    "    print(\"âœ… Document processor initialized with GPT-4o!\")\n",
    "    \n",
    "    # Process documents using GPT-4o\n",
    "    print(\"\\nğŸš€ Starting document processing with GPT-4o...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processing_results = processor.process_all_documents()\n",
    "    \n",
    "    print(\"\\nâœ… Document processing completed!\")\n",
    "else:\n",
    "    print(\"âŒ Cannot initialize processor - missing clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Summary\n",
    "\n",
    "Perfect, now let's run the following code and we will be able to check locally the transcription of our files. Please do double check if they make sense. Special attention should be given to the information extracted from the images, as it should contain deeply detailed description of the crash in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processing results\n",
    "processor.save_processed_results(processing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Cosmos our data!\n",
    "\n",
    "But first... if you inspected correctly you might have seen that we have indeed extracted data from our files, but it is not structured at all. We might as well do that! We will use the Claimant_ID on the top part of each submission (carefully processed by our teams) to create a database. And of course, to do that we will use... Generative AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "\n",
    "# Rename the model to something more appropriate\n",
    "class ClaimInfo(BaseModel):\n",
    "    claimant_id: str\n",
    "    policyholder_name: str\n",
    "    policyholder_address: str\n",
    "    policyholder_phone: str\n",
    "    policyholder_email: str\n",
    "    policy_number: str\n",
    "    vehicle_year_make_model: str\n",
    "    vehicle_color: str\n",
    "    vehicle_vin: str\n",
    "    vehicle_license_plate: str\n",
    "    incident_date: str\n",
    "    incident_time: str\n",
    "    incident_location: str\n",
    "    incident_description: str\n",
    "    damage_description: str\n",
    "    witness_name: str\n",
    "    witness_phone: str\n",
    "    police_department: str\n",
    "    police_report_number: str\n",
    "    repair_shop_name: str\n",
    "    repair_shop_address: str\n",
    "    attachments: str\n",
    "    claim_request: str\n",
    "    signature_name: str\n",
    "    signature_date: str\n",
    "\n",
    "\n",
    "def extract_structured_claim_info(text_content: str, claim_id: str) -> dict:\n",
    "    \"\"\"Extract structured information from claim text using Azure OpenAI structured outputs\"\"\"\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        \n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,  # Use your deployment name\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"\"\"You are an expert insurance claims processor. Extract structured information from crash statements and insurance claims. \n",
    "                    If any field is not available in the text, use \"N/A\" as the value. \n",
    "                    Be thorough and accurate in extracting all available information.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Extract the structured information from this crash statement for claim {claim_id}:\\n\\n{text_content}\"\n",
    "                },\n",
    "            ],\n",
    "            response_format=ClaimInfo,\n",
    "        )\n",
    "        \n",
    "        structured_data = completion.choices[0].message.parsed\n",
    "        print(f\"âœ… Extracted structured data for claim {claim_id}\")\n",
    "        return structured_data.model_dump()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting structured data for claim {claim_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_crash_reports_simplified():\n",
    "    \"\"\"Process JSON file and create simplified crash reports with only structured info and image descriptions\"\"\"\n",
    "    \n",
    "    # Load the processed JSON file\n",
    "    with open('processed_documents_for_vectorization.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    statements_lookup = {item['metadata']['file_name']: item for item in data['statements'] if item['success']}\n",
    "    claims_lookup = {item['metadata']['file_name']: item for item in data['claims'] if item['success']}\n",
    "    \n",
    "    # Define crash to claim ID mapping\n",
    "    crash_to_claim_mapping = {\n",
    "        'crash1': 'CL001',\n",
    "        'crash2': 'CL002', \n",
    "        'crash3': 'CL003',\n",
    "        'crash4': 'CL001',  # Same claim as crash1\n",
    "        'crash5': 'CL004'\n",
    "    }\n",
    "    \n",
    "    # Group by claim ID to handle multiple crashes per claim\n",
    "    claims_data = {}\n",
    "    \n",
    "    for crash_num, claim_id in crash_to_claim_mapping.items():\n",
    "        statement_file = f\"{crash_num}.md\"\n",
    "        image_files = [f\"{crash_num}.jpg\", f\"{crash_num}.jpeg\"]\n",
    "        \n",
    "        # Find matching image file\n",
    "        image_file = None\n",
    "        for img in image_files:\n",
    "            if img in claims_lookup:\n",
    "                image_file = img\n",
    "                break\n",
    "        \n",
    "        if statement_file in statements_lookup and image_file:\n",
    "            statement_data = statements_lookup[statement_file]\n",
    "            image_data = claims_lookup[image_file]\n",
    "            \n",
    "            if claim_id not in claims_data:\n",
    "                claims_data[claim_id] = {\n",
    "                    \"structured_info\": [],\n",
    "                    \"image_descriptions\": []\n",
    "                }\n",
    "            \n",
    "            # Extract structured information from the statement text\n",
    "            print(f\"ğŸ” Extracting structured info for {crash_num} (Claim: {claim_id})...\")\n",
    "            structured_info = extract_structured_claim_info(statement_data['text'], claim_id)\n",
    "            \n",
    "            if structured_info:\n",
    "                claims_data[claim_id][\"structured_info\"].append({\n",
    "                    \"crash_number\": crash_num,\n",
    "                    \"structured_data\": structured_info\n",
    "                })\n",
    "            \n",
    "            # Add image description\n",
    "            claims_data[claim_id][\"image_descriptions\"].append({\n",
    "                \"crash_number\": crash_num,\n",
    "                \"image_file\": image_file,\n",
    "                \"description\": image_data['description']\n",
    "            })\n",
    "    \n",
    "    # Create simplified crash reports with only structured info and image descriptions\n",
    "    simplified_reports = []\n",
    "    for claim_id, claim_data in claims_data.items():\n",
    "        # Combine structured info from all crashes in this claim\n",
    "        combined_structured_info = {}\n",
    "        if claim_data[\"structured_info\"]:\n",
    "            # Use the first crash's structured info as base\n",
    "            combined_structured_info = claim_data[\"structured_info\"][0][\"structured_data\"].copy()\n",
    "            \n",
    "            # For claims with multiple crashes, add them as additional crashes\n",
    "            if len(claim_data[\"structured_info\"]) > 1:\n",
    "                combined_structured_info[\"additional_crashes\"] = []\n",
    "                for i in range(1, len(claim_data[\"structured_info\"])):\n",
    "                    additional_crash = {\n",
    "                        \"crash_number\": claim_data[\"structured_info\"][i][\"crash_number\"],\n",
    "                        \"structured_data\": claim_data[\"structured_info\"][i][\"structured_data\"]\n",
    "                    }\n",
    "                    combined_structured_info[\"additional_crashes\"].append(additional_crash)\n",
    "        \n",
    "        simplified_report = {\n",
    "            \"claim_id\": claim_id,\n",
    "            \"structured_claim_info\": combined_structured_info,\n",
    "            \"image_descriptions\": claim_data[\"image_descriptions\"]\n",
    "        }\n",
    "        \n",
    "        simplified_reports.append(simplified_report)\n",
    "        crashes_count = len(claim_data[\"structured_info\"])\n",
    "        print(f\"âœ… Created simplified report for Claim ID: {claim_id} ({crashes_count} crashes)\")\n",
    "    \n",
    "    return simplified_reports\n",
    "\n",
    "def save_simplified_cosmos_db(simplified_reports):\n",
    "    \"\"\"Save simplified reports to Cosmos DB\"\"\"\n",
    "    \n",
    "    # Initialize Cosmos client\n",
    "    client = CosmosClient(Config.COSMOS_ENDPOINT, Config.COSMOS_KEY)\n",
    "    database = client.create_database_if_not_exists(id=Config.COSMOS_DATABASE)\n",
    "    \n",
    "    # Update partition key to use claim_id\n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=Config.COSMOS_CONTAINER,\n",
    "        partition_key=PartitionKey(path=\"/claim_id\")\n",
    "    )\n",
    "    \n",
    "    # Save simplified reports\n",
    "    print(\"ğŸ’¾ Saving simplified crash reports...\")\n",
    "    for report in simplified_reports:\n",
    "        try:\n",
    "            # Add required id field for Cosmos DB\n",
    "            report_with_id = report.copy()\n",
    "            report_with_id[\"id\"] = report[\"claim_id\"]\n",
    "            \n",
    "            container.upsert_item(body=report_with_id)\n",
    "            print(f\"âœ… Saved simplified Claim {report['claim_id']} to Cosmos DB\")\n",
    "            \n",
    "            # Print sample of structured info for verification\n",
    "            if report.get(\"structured_claim_info\"):\n",
    "                sample_fields = [\"policyholder_name\", \"policy_number\", \"incident_date\", \"incident_location\"]\n",
    "                print(f\"   ğŸ“‹ Sample structured data:\")\n",
    "                for field in sample_fields:\n",
    "                    value = report[\"structured_claim_info\"].get(field, \"N/A\")\n",
    "                    print(f\"      {field}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving {report['claim_id']}: {e}\")\n",
    "\n",
    "# Execute the simplified processing\n",
    "print(\"ğŸš€ Starting simplified crash report processing...\")\n",
    "simplified_reports = process_crash_reports_simplified()\n",
    "print(f\"\\nğŸ“Š Created {len(simplified_reports)} simplified crash reports\")\n",
    "\n",
    "# Display the simplified structure\n",
    "print(\"\\nğŸ“‹ Simplified Report Structure:\")\n",
    "for report in simplified_reports:\n",
    "    print(f\"  Claim {report['claim_id']}:\")\n",
    "    print(f\"    - Structured Info: {'âœ…' if report.get('structured_claim_info') else 'âŒ'}\")\n",
    "    print(f\"    - Image Descriptions: {len(report.get('image_descriptions', []))}\")\n",
    "    \n",
    "    # Show sample structured data\n",
    "    if report.get(\"structured_claim_info\"):\n",
    "        policyholder = report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\")\n",
    "        policy_num = report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\")\n",
    "        print(f\"    - Policyholder: {policyholder}, Policy: {policy_num}\")\n",
    "\n",
    "# Save to Cosmos DB\n",
    "save_simplified_cosmos_db(simplified_reports)\n",
    "\n",
    "# Save the simplified reports locally\n",
    "with open('simplified_crash_reports.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_reports, f, indent=2, ensure_ascii=False)\n",
    "print(f\"ğŸ’¾ Simplified crash reports saved to 'simplified_crash_reports.json'\")\n",
    "\n",
    "# Show a sample of what the final JSON looks like\n",
    "print(\"\\nğŸ“„ Sample of simplified JSON structure:\")\n",
    "if simplified_reports:\n",
    "    sample_report = simplified_reports[0]\n",
    "    print(json.dumps({\n",
    "        \"sample_claim\": {\n",
    "            \"claim_id\": sample_report[\"claim_id\"],\n",
    "            \"structured_claim_info\": {\n",
    "                \"policyholder_name\": sample_report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\"),\n",
    "                \"policy_number\": sample_report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\"),\n",
    "                \"incident_date\": sample_report[\"structured_claim_info\"].get(\"incident_date\", \"N/A\"),\n",
    "                \"...\": \"all other structured fields\"\n",
    "            },\n",
    "            \"image_descriptions\": [\n",
    "                {\n",
    "                    \"crash_number\": sample_report[\"image_descriptions\"][0][\"crash_number\"],\n",
    "                    \"image_file\": sample_report[\"image_descriptions\"][0][\"image_file\"],\n",
    "                    \"description\": \"Detailed image description...\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
