{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization with Azure AI Search Integrated Vectorization\n",
    "\n",
    "This notebook demonstrates how to create a sophisticated search index using Azure AI Search's integrated vectorization capabilities for insurance document retrieval. The workflow includes:\n",
    "\n",
    "1. **Retrieve Processed Documents from Azure Blob Storage**: Download the processed insurance documents (policies, claims, statements) that were created in the previous notebook, including structured claim data and detailed image descriptions.\n",
    "\n",
    "2. **Create Azure AI Search Index with Integrated Vectorization**: \n",
    "   - **Index Schema Design**: Define a comprehensive search schema with fields for content, metadata, and vector embeddings\n",
    "   - **Integrated Vectorization Setup**: Configure Azure AI Search to automatically generate embeddings using Azure OpenAI's text-embedding-ada-002 model\n",
    "   - **Semantic Search Configuration**: Enable semantic search capabilities for natural language queries\n",
    "\n",
    "3. **Intelligent Text Chunking**: Process large insurance documents into optimally-sized chunks with overlapping content to ensure comprehensive coverage while maintaining context for accurate retrieval.\n",
    "\n",
    "4. **Upload Documents to Azure AI Search**: \n",
    "   - **Batch Processing**: Efficiently upload document chunks to the search index\n",
    "   - **Automatic Embedding Generation**: Azure AI Search automatically creates vector embeddings for each document chunk using the configured OpenAI model\n",
    "   - **Real-time Indexing**: Documents become immediately searchable upon upload\n",
    "\n",
    "5. **Advanced Search Testing**: \n",
    "   - **Semantic Search**: Test natural language queries against insurance policies using AI-powered semantic understanding\n",
    "   - **Vector Search**: Perform similarity-based searches using vector embeddings\n",
    "   - **Hybrid Search**: Combine keyword and vector search for optimal results\n",
    "   - **Interactive Testing**: Provide an interactive interface for real-time search testing\n",
    "\n",
    "6. **Search Analytics and Validation**: Generate comprehensive statistics about the indexed documents, search performance, and readiness for AI agent integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "Let's start with handling the import of our libraries and load the `.env` variables that we have saved in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    ExhaustiveKnnAlgorithmConfiguration\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    \n",
    "    # Azure AI Search configuration\n",
    "    SEARCH_SERVICE_NAME = os.getenv('SEARCH_SERVICE_NAME')\n",
    "    SEARCH_SERVICE_ENDPOINT = os.getenv('SEARCH_SERVICE_ENDPOINT')\n",
    "    SEARCH_ADMIN_KEY = os.getenv('SEARCH_ADMIN_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration (for integrated vectorization)\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT', 'text-embedding-ada-002')\n",
    "    \n",
    "    # Container names\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    \n",
    "    # Search index configuration\n",
    "    SEARCH_INDEX_NAME = 'insurance-documents-index'\n",
    "    CHUNK_SIZE = 1000  # Characters per chunk\n",
    "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.SEARCH_SERVICE_ENDPOINT,\n",
    "    Config.SEARCH_ADMIN_KEY,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables:\")\n",
    "    if not Config.SEARCH_SERVICE_ENDPOINT:\n",
    "        print(\"  - SEARCH_SERVICE_ENDPOINT\")\n",
    "    if not Config.SEARCH_ADMIN_KEY:\n",
    "        print(\"  - SEARCH_ADMIN_KEY\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"üîç Search Service: {Config.SEARCH_SERVICE_NAME}\")\n",
    "    print(f\"üîó Search Endpoint: {Config.SEARCH_SERVICE_ENDPOINT}\")\n",
    "    print(f\"üì¶ Processed Documents Container: {Config.PROCESSED_CONTAINER}\")\n",
    "    print(f\"üìá Search Index: {Config.SEARCH_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Azure Services\n",
    "\n",
    "The next cell creates connections to Azure Blob Storage for document retrieval and Azure AI Search for index management, with comprehensive error handling and connection testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure AI Search clients\n",
    "        search_credential = AzureKeyCredential(Config.SEARCH_ADMIN_KEY)\n",
    "        \n",
    "        search_index_client = SearchIndexClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        search_client = SearchClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            index_name=Config.SEARCH_INDEX_NAME,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        # Test the connections\n",
    "        containers = list(blob_service_client.list_containers())\n",
    "        print(f\"‚úÖ Connected to Blob Storage - Found {len(containers)} containers\")\n",
    "        \n",
    "        # Test search service (fixed the storage_size access)\n",
    "        try:\n",
    "            service_stats = search_index_client.get_service_statistics()\n",
    "            storage_used = getattr(service_stats, 'storage_size', 'Unknown')\n",
    "            print(f\"‚úÖ Connected to Azure AI Search - Storage used: {storage_used}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úÖ Connected to Azure AI Search - Service is available\")\n",
    "            print(f\"   (Note: Could not get statistics: {e})\")\n",
    "        \n",
    "        return blob_service_client, search_index_client, search_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing clients: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize clients\n",
    "blob_service_client, search_index_client, search_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Azure AI Search Index with Integrated Vectorization\n",
    "The next cell defines a SearchIndexManager class that creates a sophisticated search index with integrated vectorization, semantic search capabilities, and proper field schema for insurance documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchIndexManager:\n",
    "    \"\"\"Class to manage Azure AI Search index with integrated vectorization\"\"\"\n",
    "    \n",
    "    def __init__(self, search_index_client: SearchIndexClient):\n",
    "        self.search_index_client = search_index_client\n",
    "        self.index_name = Config.SEARCH_INDEX_NAME\n",
    "\n",
    "    def _format_azure_openai_endpoint(self, endpoint: str) -> str:\n",
    "        \"\"\"Format the Azure OpenAI endpoint for use with Azure AI Search vectorizer\"\"\"\n",
    "        # Remove trailing slash if present\n",
    "        endpoint = endpoint.rstrip('/')\n",
    "        \n",
    "        # Check if it already has the correct format\n",
    "        if endpoint.endswith('.openai.azure.com'):\n",
    "            return endpoint\n",
    "        \n",
    "        # Extract the resource name from various possible formats\n",
    "        if '.cognitiveservices.azure.com' in endpoint:\n",
    "            # Convert from cognitive services format to OpenAI format\n",
    "            resource_name = endpoint.split('.')[0].split('//')[-1]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        elif '/openai/' in endpoint:\n",
    "            # Extract resource name from URL with /openai/ path\n",
    "            parts = endpoint.split('/')\n",
    "            resource_name = parts[2].split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        else:\n",
    "            # Try to extract resource name and format correctly\n",
    "            if 'https://' in endpoint:\n",
    "                resource_name = endpoint.split('//')[1].split('.')[0]\n",
    "            else:\n",
    "                resource_name = endpoint.split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "    \n",
    "    def create_search_index(self) -> bool:\n",
    "        \"\"\"Create a search index with integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Format the Azure OpenAI endpoint correctly\n",
    "            formatted_endpoint = self._format_azure_openai_endpoint(Config.AZURE_OPENAI_ENDPOINT)\n",
    "            print(f\"üîó Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"üîó Formatted endpoint: {formatted_endpoint}\")\n",
    "            print(f\"üöÄ Using deployment: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            \n",
    "            # Define the vectorizer for integrated vectorization\n",
    "            vectorizer = AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"insurance-vectorizer\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=formatted_endpoint,  # Use formatted endpoint\n",
    "                    deployment_name=Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,  # Use config variable\n",
    "                    model_name=\"text-embedding-ada-002\",\n",
    "                    api_key=Config.AZURE_OPENAI_API_KEY\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Define vector search configuration\n",
    "            vector_search = VectorSearch(\n",
    "                algorithms=[\n",
    "                    HnswAlgorithmConfiguration(name=\"insurance-algorithm\", kind=\"hnsw\"),\n",
    "                    ExhaustiveKnnAlgorithmConfiguration(name=\"my-eknn-vector-config\", kind=\"exhaustiveKnn\")\n",
    "                ],\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"insurance-profile\",\n",
    "                        algorithm_configuration_name=\"insurance-algorithm\",\n",
    "                        vectorizer_name=\"insurance-vectorizer\"\n",
    "                    )\n",
    "                ],\n",
    "                vectorizers=[vectorizer]\n",
    "            )\n",
    "            \n",
    "            # Define semantic search configuration\n",
    "            semantic_config = SemanticConfiguration(\n",
    "                name=\"insurance-semantic\",  # Fixed to match SearchTester\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"title\"),\n",
    "                    content_fields=[SemanticField(field_name=\"content\")],\n",
    "                    keywords_fields=[\n",
    "                        SemanticField(field_name=\"category\"),\n",
    "                        SemanticField(field_name=\"file_name\")\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            semantic_search = SemanticSearch(\n",
    "                configurations=[semantic_config]\n",
    "            )\n",
    "            \n",
    "            # Define the search index schema\n",
    "            fields = [\n",
    "                SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "                SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "                SearchableField(name=\"file_name\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"file_type\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"chunk_id\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_count\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"original_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"processing_date\", type=SearchFieldDataType.DateTimeOffset),\n",
    "                \n",
    "                # Vector field for integrated vectorization\n",
    "                SearchField(\n",
    "                    name=\"content_vector\",\n",
    "                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True,\n",
    "                    vector_search_dimensions=1536,  # ada-002 embedding dimension\n",
    "                    vector_search_profile_name=\"insurance-profile\"\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Create the search index\n",
    "            index = SearchIndex(\n",
    "                name=self.index_name,\n",
    "                fields=fields,\n",
    "                vector_search=vector_search,\n",
    "                semantic_search=semantic_search\n",
    "            )\n",
    "            \n",
    "            # Create or update the index\n",
    "            result = self.search_index_client.create_or_update_index(index)\n",
    "            print(f\"‚úÖ Search index '{self.index_name}' created successfully!\")\n",
    "            print(f\"üìã Index fields: {len(result.fields)}\")\n",
    "            print(f\"üîç Vector search enabled: {bool(result.vector_search)}\")\n",
    "            print(f\"üß† Semantic search enabled: {bool(result.semantic_search)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating search index: {e}\")\n",
    "            print(f\"üîç Debug info:\")\n",
    "            print(f\"   - Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"   - Deployment name: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            print(f\"   - API key present: {bool(Config.AZURE_OPENAI_API_KEY)}\")\n",
    "            \n",
    "            # Add more detailed error information\n",
    "            import traceback\n",
    "            print(f\"üìã Full error details:\\n{traceback.format_exc()}\")\n",
    "            return False\n",
    "    \n",
    "    def delete_index_if_exists(self) -> bool:\n",
    "        \"\"\"Delete the index if it exists\"\"\"\n",
    "        try:\n",
    "            self.search_index_client.delete_index(self.index_name)\n",
    "            print(f\"‚úÖ Deleted existing index: {self.index_name}\")\n",
    "            return True\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ÑπÔ∏è Index {self.index_name} doesn't exist - will create new\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_index_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search index\"\"\"\n",
    "        try:\n",
    "            index = self.search_index_client.get_index(self.index_name)\n",
    "            stats = self.search_index_client.get_index_statistics(self.index_name)\n",
    "            \n",
    "            # Handle both object and dictionary responses\n",
    "            if hasattr(stats, 'document_count'):\n",
    "                # Object response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.document_count,\n",
    "                    \"storage_size\": stats.storage_size,\n",
    "                    \"vector_index_size\": getattr(stats, 'vector_index_size', 0)\n",
    "                }\n",
    "            else:\n",
    "                # Dictionary response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.get('document_count', 0),\n",
    "                    \"storage_size\": stats.get('storage_size', 0),\n",
    "                    \"vector_index_size\": stats.get('vector_index_size', 0)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting index stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialize search index manager\n",
    "if search_index_client:\n",
    "    index_manager = SearchIndexManager(search_index_client)\n",
    "    \n",
    "    # Option to recreate index (uncomment if needed)\n",
    "    # print(\"üîÑ Recreating search index...\")\n",
    "    # index_manager.delete_index_if_exists()\n",
    "    \n",
    "    success = index_manager.create_search_index()\n",
    "    if success:\n",
    "        print(\"\\nüìä Index created successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to create search index\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create search index - missing search client\")\n",
    "    index_manager = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Retrieval and Processing\n",
    "\n",
    "The next cell defines two essential classes: DocumentRetriever handles downloading processed documents from Azure Blob Storage, while TextChunker intelligently splits large documents into optimally-sized chunks with overlapping content. These components prepare the insurance documents for efficient indexing and retrieval in the search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the DocumentRetriever class from previous notebook\n",
    "class DocumentRetriever:\n",
    "    \"\"\"Class to handle document retrieval from blob storage\"\"\"\n",
    "    \n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_all_processed_documents(self) -> Dict:\n",
    "        \"\"\"Get all processed documents ready for vectorization\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(Config.PROCESSED_CONTAINER)\n",
    "            blob_client = container_client.get_blob_client(\"processed_documents_for_vectorization.json\")\n",
    "            \n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            documents = json.loads(blob_data.decode('utf-8'))\n",
    "            \n",
    "            print(f\"‚úÖ Downloaded processed documents\")\n",
    "            return documents\n",
    "                \n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ùå File not found: processed_documents_for_vectorization.json\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading documents: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Text chunking class (simplified for search index)\n",
    "class TextChunker:\n",
    "    \"\"\"Class to handle intelligent text chunking for search index\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text_for_search(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Create chunks optimized for search index\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        chunks = []\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'chunk_id': 0,\n",
    "                'chunk_count': 1,\n",
    "                'metadata': metadata.copy()\n",
    "            }]\n",
    "        \n",
    "        # Simple sliding window chunking\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            \n",
    "            # Try to break at sentence boundaries\n",
    "            if end < len(text):\n",
    "                sentence_end = text.rfind('.', start, end)\n",
    "                if sentence_end > start:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'content': chunk_text,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_count': 0,  # Will be updated later\n",
    "                    'metadata': metadata.copy()\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = max(start + self.chunk_size - self.chunk_overlap, end)\n",
    "        \n",
    "        # Update chunk count\n",
    "        for chunk in chunks:\n",
    "            chunk['chunk_count'] = len(chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize processors\n",
    "if blob_service_client:\n",
    "    retriever = DocumentRetriever(blob_service_client)\n",
    "    chunker = TextChunker(\n",
    "        chunk_size=Config.CHUNK_SIZE,\n",
    "        chunk_overlap=Config.CHUNK_OVERLAP\n",
    "    )\n",
    "    print(\"‚úÖ Document processors initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve and Process Documents\n",
    "\n",
    "The next cell implements an enhanced document retrieval system that downloads processed insurance documents from Azure Blob Storage and prepares them for search indexing with detailed error handling and debugging capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced document retriever with better error handling and debugging\n",
    "class EnhancedDocumentRetriever:\n",
    "    \"\"\"Enhanced class to handle document retrieval with detailed debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_all_processed_documents(self) -> Dict:\n",
    "        \"\"\"Get all processed documents with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"üîç Attempting to retrieve from container: {Config.PROCESSED_CONTAINER}\")\n",
    "            \n",
    "            # Get container client\n",
    "            container_client = self.blob_service_client.get_container_client(Config.PROCESSED_CONTAINER)\n",
    "            \n",
    "            # Try to access the specific file\n",
    "            blob_name = \"processed_documents_for_vectorization.json\"\n",
    "            print(f\"üì• Downloading file: {blob_name}\")\n",
    "            \n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "            \n",
    "            # Check if blob exists first\n",
    "            try:\n",
    "                blob_props = blob_client.get_blob_properties()\n",
    "                file_size = blob_props.size\n",
    "                print(f\"‚úÖ File found - Size: {file_size / (1024*1024):.2f} MB\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå File access error: {e}\")\n",
    "                return {}\n",
    "            \n",
    "            # Download the blob\n",
    "            print(\"üì• Downloading blob content...\")\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            \n",
    "            # Parse JSON\n",
    "            print(\"üîÑ Parsing JSON content...\")\n",
    "            documents = json.loads(blob_data.decode('utf-8'))\n",
    "            \n",
    "            print(f\"‚úÖ Successfully downloaded and parsed processed documents\")\n",
    "            print(f\"üìä Found categories: {list(documents.keys())}\")\n",
    "            \n",
    "            # Show some stats\n",
    "            for category, docs in documents.items():\n",
    "                successful_docs = [d for d in docs if d.get('success', False)]\n",
    "                print(f\"   - {category}: {len(successful_docs)}/{len(docs)} successful documents\")\n",
    "            \n",
    "            return documents\n",
    "                \n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ùå File not found: {blob_name}\")\n",
    "            print(f\"   Container: {Config.PROCESSED_CONTAINER}\")\n",
    "            print(\"   This means the file doesn't exist in the specified container\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå JSON parsing error: {e}\")\n",
    "            print(\"   The file exists but contains invalid JSON\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error downloading documents: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            print(f\"   Full traceback: {traceback.format_exc()}\")\n",
    "            return {}\n",
    "\n",
    "# Replace the original retriever and try document retrieval\n",
    "if blob_service_client:\n",
    "    retriever = EnhancedDocumentRetriever(blob_service_client)\n",
    "    print(\"‚úÖ Enhanced document retriever initialized\")\n",
    "    \n",
    "    # Now try to retrieve the documents\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì• RETRIEVING PROCESSED DOCUMENTS FROM BLOB STORAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    processed_documents = retriever.get_all_processed_documents()\n",
    "    \n",
    "    if processed_documents:\n",
    "        print(f\"\\nüéâ SUCCESS! Retrieved processed documents from blob storage\")\n",
    "        print(f\"üìä Available categories: {list(processed_documents.keys())}\")\n",
    "        \n",
    "        # Filter to only process POLICIES\n",
    "        policies_only = {'policies': processed_documents.get('policies', [])}\n",
    "        \n",
    "        print(f\"üéØ Filtering to process POLICIES only...\")\n",
    "        print(f\"üìÑ Found {len(policies_only['policies'])} policy documents\")\n",
    "        \n",
    "        # Process only policy documents into search-ready chunks\n",
    "        search_documents = []\n",
    "        \n",
    "        for category, docs in policies_only.items():\n",
    "            print(f\"\\nüìÇ Processing {category} documents...\")\n",
    "            \n",
    "            successful_docs = [doc for doc in docs if doc.get('success', False)]\n",
    "            print(f\"‚úÖ Processing {len(successful_docs)} successful {category} documents\")\n",
    "            \n",
    "            for doc in tqdm(successful_docs, desc=f\"Processing {category}\"):\n",
    "                # Get text content from policies (markdown files)\n",
    "                text_content = doc.get('text', '')\n",
    "                if not text_content:\n",
    "                    print(f\"‚ö†Ô∏è Skipping document with no text content: {doc.get('metadata', {}).get('file_name', 'Unknown')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare metadata\n",
    "                metadata = doc.get('metadata', {}).copy()\n",
    "                metadata['category'] = category\n",
    "                \n",
    "                # Create chunks for this document\n",
    "                chunks = chunker.chunk_text_for_search(text_content, metadata)\n",
    "                \n",
    "                # Convert chunks to search documents\n",
    "                for chunk in chunks:\n",
    "                    search_doc = {\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'title': f\"{metadata.get('file_name', 'Unknown')} - Part {chunk['chunk_id'] + 1}\",\n",
    "                        'content': chunk['content'],\n",
    "                        'category': category,\n",
    "                        'file_name': metadata.get('file_name', 'Unknown'),\n",
    "                        'file_type': metadata.get('file_type', 'markdown'),\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'chunk_count': chunk['chunk_count'],\n",
    "                        'original_length': len(text_content),\n",
    "                        'chunk_length': len(chunk['content']),\n",
    "                        'processing_date': datetime.now().isoformat() + 'Z'\n",
    "                    }\n",
    "                    search_documents.append(search_doc)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Prepared {len(search_documents)} policy document chunks for search index\")\n",
    "        \n",
    "        # Show detailed statistics for policies only\n",
    "        if search_documents:\n",
    "            total_files = len(set(doc['file_name'] for doc in search_documents))\n",
    "            total_chunks = len(search_documents)\n",
    "            avg_chunk_length = sum(doc['chunk_length'] for doc in search_documents) / total_chunks\n",
    "            \n",
    "            print(f\"\\nüìä POLICIES INDEXING SUMMARY:\")\n",
    "            print(f\"   üìÑ Total policy files: {total_files}\")\n",
    "            print(f\"   üóÇÔ∏è Total chunks created: {total_chunks}\")\n",
    "            print(f\"   üìè Average chunk length: {avg_chunk_length:.0f} characters\")\n",
    "            \n",
    "            # Show file breakdown\n",
    "            file_stats = {}\n",
    "            for doc in search_documents:\n",
    "                file_name = doc['file_name']\n",
    "                if file_name not in file_stats:\n",
    "                    file_stats[file_name] = 0\n",
    "                file_stats[file_name] += 1\n",
    "            \n",
    "            print(f\"\\nüìã Policy files breakdown:\")\n",
    "            for file_name, chunk_count in file_stats.items():\n",
    "                print(f\"   ‚Ä¢ {file_name}: {chunk_count} chunks\")\n",
    "        else:\n",
    "            print(\"‚ùå No policy documents were processed successfully\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Still unable to retrieve documents from blob storage\")\n",
    "        print(\"üí° Troubleshooting steps:\")\n",
    "        print(\"   1. Verify the file exists in blob storage using Azure Portal\") \n",
    "        print(\"   2. Check that the container name 'processed-documents' is correct\")\n",
    "        print(\"   3. Ensure your storage connection string has the right permissions\")\n",
    "        print(\"   4. Try running the document processing notebook (1.document-processing.ipynb) first\")\n",
    "        search_documents = []\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed - blob service client not available\")\n",
    "    search_documents = []\n",
    "\n",
    "print(f\"\\nüîç Final check - search_documents variable has {len(search_documents) if 'search_documents' in locals() else 0} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Documents to Azure AI Search with Integrated Vectorization\n",
    "\n",
    "The next cell implements a SearchIndexUploader class that efficiently uploads the processed policy document chunks to Azure AI Search in batches, with automatic embedding generation through integrated vectorization and comprehensive error handling and progress tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchIndexUploader:\n",
    "    \"\"\"Class to upload documents to Azure AI Search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def upload_documents_batch(self, documents: List[Dict], batch_size: int = 50) -> bool:\n",
    "        \"\"\"Upload documents to search index in batches\"\"\"\n",
    "        try:\n",
    "            total_docs = len(documents)\n",
    "            print(f\"üì§ Uploading {total_docs} documents to search index...\")\n",
    "            \n",
    "            # Upload in batches\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"Uploading batches\"):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                \n",
    "                # Prepare batch for upload (Azure AI Search will handle vectorization)\n",
    "                upload_batch = []\n",
    "                for doc in batch:\n",
    "                    # Remove any fields that shouldn't be in the search document\n",
    "                    search_doc = doc.copy()\n",
    "                    upload_batch.append(search_doc)\n",
    "                \n",
    "                # Upload batch\n",
    "                result = self.search_client.upload_documents(documents=upload_batch)\n",
    "                \n",
    "                # Check for errors\n",
    "                failed_docs = [r for r in result if not r.succeeded]\n",
    "                if failed_docs:\n",
    "                    print(f\"‚ö†Ô∏è Failed to upload {len(failed_docs)} documents in batch {i//batch_size + 1}\")\n",
    "                    for failed in failed_docs[:3]:  # Show first 3 errors\n",
    "                        print(f\"   Error: {failed.error_message}\")\n",
    "            \n",
    "            print(f\"‚úÖ Document upload completed!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the current document count in the index\"\"\"\n",
    "        try:\n",
    "            # Simple search to get document count\n",
    "            results = self.search_client.search(\"*\", include_total_count=True, top=1)\n",
    "            return results.get_count()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting document count: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Upload documents to search index\n",
    "if search_client and search_documents:\n",
    "    uploader = SearchIndexUploader(search_client)\n",
    "    \n",
    "    print(\"\\nüöÄ Starting POLICIES upload to Azure AI Search...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ Uploading POLICY documents only\")\n",
    "    print(\"‚ÑπÔ∏è Azure AI Search will automatically generate embeddings using integrated vectorization\")\n",
    "    \n",
    "    success = uploader.upload_documents_batch(search_documents)\n",
    "    \n",
    "    if success:\n",
    "        # Wait a moment for indexing to complete\n",
    "        import time\n",
    "        print(\"\\n‚è≥ Waiting for indexing to complete...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Get final document count\n",
    "        doc_count = uploader.get_document_count()\n",
    "        print(f\"‚úÖ Index now contains {doc_count} policy document chunks\")\n",
    "        \n",
    "        # Get index statistics\n",
    "        if index_manager:\n",
    "            stats = index_manager.get_index_stats()\n",
    "            if stats:\n",
    "                print(f\"üìä Index statistics:\")\n",
    "                print(f\"   - Policy documents: {stats.get('document_count', 'N/A')}\")\n",
    "                print(f\"   - Storage size: {stats.get('storage_size', 'N/A')} bytes\")\n",
    "                print(f\"   - Vector index size: {stats.get('vector_index_size', 'N/A')} bytes\")\n",
    "        \n",
    "        print(f\"\\nüéØ SUCCESS: Only policy documents have been indexed!\")\n",
    "        print(f\"üìÑ Your Azure AI Search index now contains comprehensive policy information\")\n",
    "        print(f\"üîç Ready for policy-related queries and AI agent integration\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to upload policy documents to search index\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot upload documents - missing search client or policy documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Search Index with Semantic and Vector Search\n",
    "\n",
    "The next cell defines a SearchTester class that provides comprehensive testing capabilities for the Azure AI Search index, including semantic search with reranking, hybrid search combining keyword and vector approaches, and formatted result display with relevance scores and content previews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTester:\n",
    "    \"\"\"Class to test the search index with various query types\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def vector_search(self, query: str, top_k: int = 5, category_filter: str = None) -> List[Dict]:\n",
    "        \"\"\"Perform vector search using integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Build search parameters\n",
    "            search_params = {\n",
    "                \"search_text\": query,\n",
    "                \"top\": top_k,\n",
    "                \"search_mode\": \"any\",\n",
    "                \"query_type\": \"semantic\",\n",
    "                \"semantic_configuration_name\": \"insurance-semantic\",\n",
    "                \"select\": [\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\", \"chunk_count\"]\n",
    "            }\n",
    "            \n",
    "            # Add category filter if specified\n",
    "            if category_filter:\n",
    "                search_params[\"filter\"] = f\"category eq '{category_filter}'\"\n",
    "            \n",
    "            # Perform search\n",
    "            results = self.search_client.search(**search_params)\n",
    "            \n",
    "            # Convert results to list\n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'chunk_count': result['chunk_count'],\n",
    "                    'score': result.get('@search.score', 0),\n",
    "                    'reranker_score': result.get('@search.reranker_score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in vector search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search (keyword + vector)\"\"\"\n",
    "        try:\n",
    "            results = self.search_client.search(\n",
    "                search_text=query,\n",
    "                top=top_k,\n",
    "                search_mode=\"all\",\n",
    "                include_total_count=True,\n",
    "                select=[\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\"]\n",
    "            )\n",
    "            \n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'score': result.get('@search.score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in hybrid search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def display_search_results(self, query: str, results: List[Dict], search_type: str = \"Search\"):\n",
    "        \"\"\"Display search results in a formatted way\"\"\"\n",
    "        print(f\"\\nüîç {search_type} Results for: '{query}'\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            score = result.get('score', 0)\n",
    "            reranker_score = result.get('reranker_score', 0)\n",
    "            \n",
    "            print(f\"\\n{i}. üìÑ {result['title']}\")\n",
    "            print(f\"   üìÇ Category: {result['category']}\")\n",
    "            print(f\"   üìä Score: {score:.4f}\", end=\"\")\n",
    "            if reranker_score > 0:\n",
    "                print(f\" | Reranker: {reranker_score:.4f}\")\n",
    "            else:\n",
    "                print()\n",
    "            print(f\"   üìù Chunk {result['chunk_id'] + 1}\")\n",
    "            \n",
    "            # Show preview of content\n",
    "            preview = result['content'][:300]\n",
    "            if len(result['content']) > 300:\n",
    "                preview += \"...\"\n",
    "            print(f\"   üí¨ Preview: {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Initialize search tester\n",
    "if search_client:\n",
    "    search_tester = SearchTester(search_client)\n",
    "    print(\"‚úÖ Search tester initialized\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize search tester - missing search client\")\n",
    "    search_tester = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with Sample Insurance Queries\n",
    "\n",
    "The next cell executes a comprehensive test suite using predefined insurance-related queries to validate the search index functionality, demonstrating semantic search capabilities across various insurance topics like collision coverage, liability limits, and policy requirements with detailed result analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search index with sample queries\n",
    "if search_tester:\n",
    "    test_queries = [\n",
    "        \"What is covered under collision insurance?\",\n",
    "        \"How much does comprehensive coverage cost?\", \n",
    "        \"What are the liability limits for commercial vehicles?\",\n",
    "        \"Does my policy cover theft and vandalism?\",\n",
    "        \"What happens if I hit an uninsured driver?\",\n",
    "        \"High value vehicle insurance requirements\",\n",
    "        \"Motorcycle insurance coverage options\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing Azure AI Search with integrated vectorization...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\nüîç Testing query: '{query}'\")\n",
    "        \n",
    "        # Test semantic search\n",
    "        results = search_tester.vector_search(query, top_k=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"‚úÖ Found {len(results)} relevant chunks\")\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "        else:\n",
    "            print(\"‚ùå No relevant documents found\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n‚úÖ Query testing completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot test queries - search tester not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Search Interface\n",
    "\n",
    "The next cell provides an interactive search function that creates a user-friendly command-line interface for real-time testing of the Azure AI Search index, allowing users to enter natural language queries, apply category filters, and compare semantic versus hybrid search results interactively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_search():\n",
    "    \"\"\"Interactive search interface for testing\"\"\"\n",
    "    if not search_tester:\n",
    "        print(\"‚ùå Search tester not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîç Interactive Azure AI Search Interface\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter your search queries (type 'quit' to exit)\")\n",
    "    print(\"Optional commands:\")\n",
    "    print(\"  - Add 'category:policies' or 'category:claims' to filter results\")\n",
    "    print(\"  - Use natural language queries for best semantic search results\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nüîç Search: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # Check for category filter\n",
    "            category_filter = None\n",
    "            if 'category:' in query:\n",
    "                parts = query.split('category:')\n",
    "                query = parts[0].strip()\n",
    "                category_filter = parts[1].strip()\n",
    "            \n",
    "            # Perform semantic search\n",
    "            print(f\"\\nüß† Performing semantic search...\")\n",
    "            results = search_tester.vector_search(query, top_k=5, category_filter=category_filter)\n",
    "            \n",
    "            # Display results\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "            \n",
    "            # Also try hybrid search for comparison\n",
    "            print(f\"\\nüîÑ Hybrid search results:\")\n",
    "            hybrid_results = search_tester.hybrid_search(query, top_k=3)\n",
    "            if hybrid_results:\n",
    "                for i, result in enumerate(hybrid_results[:2], 1):  # Show top 2\n",
    "                    print(f\"{i}. {result['title']} (Score: {result['score']:.4f})\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\nüëã Search session ended\")\n",
    "\n",
    "# Note: Uncomment the line below to start interactive search\n",
    "# interactive_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "The next cell generates a comprehensive summary of the entire Azure AI Search integration process, collecting index statistics, document counts, and search capabilities to provide a detailed final report of what was accomplished and confirm the system's readiness for AI agent integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "def generate_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the Azure AI Search integration\"\"\"\n",
    "    \n",
    "    # Get index statistics\n",
    "    index_stats = {}\n",
    "    doc_count = 0\n",
    "    \n",
    "    if search_client and index_manager:\n",
    "        try:\n",
    "            doc_count = SearchIndexUploader(search_client).get_document_count()\n",
    "            index_stats = index_manager.get_index_stats()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    summary = {\n",
    "        \"integration_summary\": {\n",
    "            \"completion_date\": datetime.now().isoformat(),\n",
    "            \"search_service\": Config.SEARCH_SERVICE_NAME,\n",
    "            \"search_index\": Config.SEARCH_INDEX_NAME,\n",
    "            \"total_documents_processed\": len(processed_documents.get('policies', []) + processed_documents.get('claims', [])) if 'processed_documents' in globals() and processed_documents else 0,\n",
    "            \"total_chunks_indexed\": doc_count,\n",
    "            \"embedding_model\": Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "            \"vectorization_method\": \"Azure AI Search Integrated Vectorization\",\n",
    "            \"chunk_configuration\": {\n",
    "                \"chunk_size\": Config.CHUNK_SIZE,\n",
    "                \"chunk_overlap\": Config.CHUNK_OVERLAP\n",
    "            }\n",
    "        },\n",
    "        \"search_capabilities\": {\n",
    "            \"semantic_search\": True,\n",
    "            \"vector_search\": True,\n",
    "            \"hybrid_search\": True,\n",
    "            \"automatic_vectorization\": True,\n",
    "            \"real_time_indexing\": True\n",
    "        },\n",
    "        \"index_statistics\": index_stats,\n",
    "        \"ready_for_ai_agents\": bool(doc_count > 0)\n",
    "    }\n",
    "    \n",
    "    # Add category breakdown if available\n",
    "    if 'search_documents' in globals() and search_documents:\n",
    "        category_stats = {}\n",
    "        for doc in search_documents:\n",
    "            category = doc['category']\n",
    "            if category not in category_stats:\n",
    "                category_stats[category] = {\n",
    "                    'chunks': 0,\n",
    "                    'total_characters': 0,\n",
    "                    'files': set()\n",
    "                }\n",
    "            \n",
    "            category_stats[category]['chunks'] += 1\n",
    "            category_stats[category]['total_characters'] += doc['chunk_length']\n",
    "            category_stats[category]['files'].add(doc['file_name'])\n",
    "        \n",
    "        # Convert sets to lists and add unique file counts\n",
    "        for category in category_stats:\n",
    "            category_stats[category]['files'] = list(category_stats[category]['files'])\n",
    "            category_stats[category]['unique_files'] = len(category_stats[category]['files'])\n",
    "        \n",
    "        summary['categories_processed'] = category_stats\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display summary\n",
    "final_summary = generate_summary()\n",
    "\n",
    "print(\"üìã AZURE AI SEARCH INTEGRATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Completed: {final_summary['integration_summary']['completion_date']}\")\n",
    "print(f\"üîç Search Service: {final_summary['integration_summary']['search_service']}\")\n",
    "print(f\"üìá Search Index: {final_summary['integration_summary']['search_index']}\")\n",
    "print(f\"üìÑ Documents Processed: {final_summary['integration_summary']['total_documents_processed']}\")\n",
    "print(f\"üóÇÔ∏è Chunks Indexed: {final_summary['integration_summary']['total_chunks_indexed']}\")\n",
    "print(f\"ü§ñ Embedding Model: {final_summary['integration_summary']['embedding_model']}\")\n",
    "print(f\"‚ö° Vectorization: {final_summary['integration_summary']['vectorization_method']}\")\n",
    "\n",
    "print(\"\\nüöÄ SEARCH CAPABILITIES:\")\n",
    "capabilities = final_summary['search_capabilities']\n",
    "for capability, enabled in capabilities.items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {capability.replace('_', ' ').title()}\")\n",
    "\n",
    "if 'categories_processed' in final_summary:\n",
    "    print(\"\\nüìä BY CATEGORY:\")\n",
    "    for category, stats in final_summary['categories_processed'].items():\n",
    "        print(f\"  ‚Ä¢ {category.title()}:\")\n",
    "        print(f\"    - Files: {stats['unique_files']}\")\n",
    "        print(f\"    - Chunks: {stats['chunks']}\")\n",
    "        print(f\"    - Total Characters: {stats['total_characters']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
